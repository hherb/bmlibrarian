"""
Base Agent Class for BMLibrarian AI Agents

Provides common functionality for all AI agents including:
- Ollama connection management
- Model configuration
- Callback system for progress updates
- Error handling patterns
- Connection testing utilities
- Queue integration for large-scale processing
- Performance metrics tracking (execution time, token usage)
"""

import json
import logging
import time
import ollama
from dataclasses import dataclass, field
from typing import Optional, Callable, Dict, Any, TYPE_CHECKING
from abc import ABC, abstractmethod


# Constants for nanosecond to second conversion
NANOSECONDS_PER_SECOND = 1_000_000_000


@dataclass
class PerformanceMetrics:
    """
    Performance metrics for tracking agent execution statistics.

    Tracks cumulative metrics across all LLM calls made by an agent,
    including token counts, timing information, and request statistics.

    Attributes:
        total_prompt_tokens: Total number of tokens sent to the model (input)
        total_completion_tokens: Total number of tokens generated by the model (output)
        total_tokens: Combined total of prompt and completion tokens
        total_requests: Number of successful LLM requests made
        total_retries: Number of retry attempts across all requests
        total_wall_time_seconds: Wall clock time for all operations (seconds)
        total_model_time_seconds: Time spent in model inference (seconds)
        total_prompt_eval_seconds: Time spent processing prompts (seconds)
        start_time: Timestamp when metrics collection started
        end_time: Timestamp when metrics collection ended (if completed)
    """
    total_prompt_tokens: int = 0
    total_completion_tokens: int = 0
    total_tokens: int = 0
    total_requests: int = 0
    total_retries: int = 0
    total_wall_time_seconds: float = 0.0
    total_model_time_seconds: float = 0.0
    total_prompt_eval_seconds: float = 0.0
    start_time: Optional[float] = None
    end_time: Optional[float] = None

    def add_request_metrics(
        self,
        prompt_tokens: int,
        completion_tokens: int,
        wall_time_seconds: float,
        model_time_ns: int = 0,
        prompt_eval_ns: int = 0,
        retries: int = 0
    ) -> None:
        """
        Add metrics from a single LLM request.

        Args:
            prompt_tokens: Number of tokens in the prompt (input)
            completion_tokens: Number of tokens in the response (output)
            wall_time_seconds: Wall clock time for the request (seconds)
            model_time_ns: Total model inference time in nanoseconds
            prompt_eval_ns: Prompt evaluation time in nanoseconds
            retries: Number of retry attempts for this request
        """
        self.total_prompt_tokens += prompt_tokens
        self.total_completion_tokens += completion_tokens
        self.total_tokens += (prompt_tokens + completion_tokens)
        self.total_requests += 1
        self.total_retries += retries
        self.total_wall_time_seconds += wall_time_seconds
        self.total_model_time_seconds += model_time_ns / NANOSECONDS_PER_SECOND
        self.total_prompt_eval_seconds += prompt_eval_ns / NANOSECONDS_PER_SECOND

    def mark_start(self) -> None:
        """Mark the start of a metrics collection period."""
        self.start_time = time.time()
        self.end_time = None

    def mark_end(self) -> None:
        """Mark the end of a metrics collection period."""
        self.end_time = time.time()

    @property
    def elapsed_time_seconds(self) -> float:
        """Get elapsed time from start to end (or current time if not ended)."""
        if self.start_time is None:
            return 0.0
        end = self.end_time if self.end_time is not None else time.time()
        return end - self.start_time

    @property
    def tokens_per_second(self) -> float:
        """Calculate average token generation rate (completion tokens per second)."""
        if self.total_model_time_seconds > 0:
            return self.total_completion_tokens / self.total_model_time_seconds
        return 0.0

    @property
    def average_tokens_per_request(self) -> float:
        """Calculate average tokens per request."""
        if self.total_requests > 0:
            return self.total_tokens / self.total_requests
        return 0.0

    def to_dict(self) -> Dict[str, Any]:
        """
        Convert metrics to a dictionary for serialization or logging.

        Returns:
            Dictionary containing all metrics fields
        """
        return {
            'total_prompt_tokens': self.total_prompt_tokens,
            'total_completion_tokens': self.total_completion_tokens,
            'total_tokens': self.total_tokens,
            'total_requests': self.total_requests,
            'total_retries': self.total_retries,
            'total_wall_time_seconds': round(self.total_wall_time_seconds, 3),
            'total_model_time_seconds': round(self.total_model_time_seconds, 3),
            'total_prompt_eval_seconds': round(self.total_prompt_eval_seconds, 3),
            'elapsed_time_seconds': round(self.elapsed_time_seconds, 3),
            'tokens_per_second': round(self.tokens_per_second, 2),
            'average_tokens_per_request': round(self.average_tokens_per_request, 1),
            'start_time': self.start_time,
            'end_time': self.end_time
        }

    def reset(self) -> None:
        """Reset all metrics to initial values."""
        self.total_prompt_tokens = 0
        self.total_completion_tokens = 0
        self.total_tokens = 0
        self.total_requests = 0
        self.total_retries = 0
        self.total_wall_time_seconds = 0.0
        self.total_model_time_seconds = 0.0
        self.total_prompt_eval_seconds = 0.0
        self.start_time = None
        self.end_time = None

if TYPE_CHECKING:
    from .orchestrator import AgentOrchestrator


logger = logging.getLogger(__name__)


class BaseAgent(ABC):
    """
    Base class for all BMLibrarian AI agents.
    
    Provides common functionality including Ollama integration,
    callback management, and standardized error handling.
    """
    
    def __init__(
        self,
        model: str,
        host: str = "http://localhost:11434",
        temperature: float = 0.1,
        top_p: float = 0.9,
        callback: Optional[Callable[[str, str], None]] = None,
        orchestrator: Optional["AgentOrchestrator"] = None,
        show_model_info: bool = True
    ):
        """
        Initialize the base agent.
        
        Args:
            model: The name of the Ollama model to use
            host: The Ollama server host URL (default: http://localhost:11434)
            temperature: Model temperature for response randomness (0.0-1.0)
            top_p: Model top-p sampling parameter (0.0-1.0)
            callback: Optional callback function called with (step, data) for progress updates
            orchestrator: Optional orchestrator for queue-based processing
            show_model_info: Whether to display model information on initialization
        """
        self.model = model
        self.host = host
        self.temperature = temperature
        self.top_p = top_p
        self.callback = callback
        self.client = ollama.Client(host=host)
        self.orchestrator = orchestrator

        # Initialize performance metrics tracking
        self._metrics = PerformanceMetrics()

        # Display model information if requested
        if show_model_info:
            self._display_model_info()
    
    def _display_model_info(self) -> None:
        """Display model information to terminal."""
        agent_type = self.get_agent_type() if hasattr(self, 'get_agent_type') else "Agent"
        print(f"ðŸ¤– {agent_type} initialized with model: {self.model}")
    
    def _call_callback(self, step: str, data: str) -> None:
        """
        Call the callback function if provided.
        
        Args:
            step: The current processing step
            data: Data associated with the step
        """
        if self.callback:
            try:
                self.callback(step, data)
            except Exception as e:
                logger.warning(f"Callback function failed for step '{step}': {e}")
    
    def _get_ollama_options(self, **overrides) -> Dict[str, Any]:
        """
        Get Ollama options with defaults and overrides.
        
        Args:
            **overrides: Option overrides to apply
            
        Returns:
            Dictionary of Ollama options
        """
        options = {
            'temperature': self.temperature,
            'top_p': self.top_p,
            'num_predict': getattr(self, 'max_tokens', 1000)  # Use config max_tokens or reasonable default
        }
        options.update(overrides)
        return options
    
    def _make_ollama_request(
        self,
        messages: list,
        system_prompt: Optional[str] = None,
        max_retries: int = 3,
        retry_delay: float = 1.0,
        **ollama_options
    ) -> str:
        """
        Make a request to Ollama with standardized error handling and retry logic.

        Args:
            messages: List of message dictionaries for the conversation
            system_prompt: Optional system prompt to prepend
            max_retries: Maximum number of retry attempts for transient failures (default: 3)
            retry_delay: Initial delay between retries in seconds (default: 1.0, doubles each retry)
            **ollama_options: Additional Ollama options

        Returns:
            The model's response content

        Raises:
            ConnectionError: If unable to connect to Ollama after retries
            ValueError: If the response is invalid after retries
        """
        agent_logger = logging.getLogger('bmlibrarian.agents')

        last_exception = None
        current_delay = retry_delay

        for attempt in range(max_retries):
            start_time = time.time()

            # Log the request (include attempt number if retrying)
            log_msg = f"Ollama request to {self.model}"
            if attempt > 0:
                log_msg += f" (retry {attempt}/{max_retries - 1})"

            agent_logger.info(log_msg, extra={'structured_data': {
                'event_type': 'agent_ollama_request',
                'agent_type': self.get_agent_type(),
                'model': self.model,
                'message_count': len(messages),
                'has_system_prompt': system_prompt is not None,
                'options': self._get_ollama_options(**ollama_options),
                'attempt': attempt + 1,
                'max_retries': max_retries,
                'timestamp': start_time
            }})

            try:
                # Prepend system message if provided
                request_messages = messages
                if system_prompt:
                    request_messages = [{'role': 'system', 'content': system_prompt}] + messages

                # Get options with any overrides
                options = self._get_ollama_options(**ollama_options)

                response = self.client.chat(
                    model=self.model,
                    messages=request_messages,
                    options=options
                )

                content = response['message']['content']
                if not content or not content.strip():
                    # Empty response is a transient failure - retry
                    raise ValueError("Empty response from model")

                # Log successful response
                response_time = (time.time() - start_time) * 1000
                wall_time_seconds = response_time / 1000.0
                success_msg = f"Ollama response received in {response_time:.2f}ms"
                if attempt > 0:
                    success_msg += f" (succeeded on retry {attempt})"

                # Extract token counts and timing from response
                prompt_tokens = response.get('prompt_eval_count', 0)
                completion_tokens = response.get('eval_count', 0)
                model_time_ns = response.get('eval_duration', 0)
                prompt_eval_ns = response.get('prompt_eval_duration', 0)

                # Track performance metrics
                self._metrics.add_request_metrics(
                    prompt_tokens=prompt_tokens,
                    completion_tokens=completion_tokens,
                    wall_time_seconds=wall_time_seconds,
                    model_time_ns=model_time_ns,
                    prompt_eval_ns=prompt_eval_ns,
                    retries=attempt
                )

                agent_logger.info(success_msg, extra={'structured_data': {
                    'event_type': 'agent_ollama_response',
                    'agent_type': self.get_agent_type(),
                    'model': self.model,
                    'response_length': len(content),
                    'response_time_ms': response_time,
                    'response_content': content[:500] + '...' if len(content) > 500 else content,
                    'prompt_tokens': prompt_tokens,
                    'completion_tokens': completion_tokens,
                    'attempt': attempt + 1,
                    'timestamp': time.time()
                }})

                return content.strip()

            except (ollama.ResponseError, ValueError) as e:
                response_time = (time.time() - start_time) * 1000
                last_exception = e

                # Determine if we should retry
                is_retryable = isinstance(e, ValueError) or 'timeout' in str(e).lower() or 'connection' in str(e).lower()

                if attempt < max_retries - 1 and is_retryable:
                    # Transient failure - will retry
                    agent_logger.warning(
                        f"Transient error in Ollama request after {response_time:.2f}ms, retrying in {current_delay:.1f}s: {e}",
                        extra={'structured_data': {
                            'event_type': 'agent_ollama_retry',
                            'agent_type': self.get_agent_type(),
                            'model': self.model,
                            'error_type': type(e).__name__,
                            'error_message': str(e),
                            'response_time_ms': response_time,
                            'attempt': attempt + 1,
                            'retry_delay_s': current_delay,
                            'timestamp': time.time()
                        }}
                    )

                    # Wait before retrying (exponential backoff)
                    time.sleep(current_delay)
                    current_delay *= 2  # Double the delay for next retry

                else:
                    # Final attempt failed or non-retryable error
                    agent_logger.error(
                        f"Ollama request failed after {response_time:.2f}ms (attempt {attempt + 1}/{max_retries}): {e}",
                        extra={'structured_data': {
                            'event_type': 'agent_ollama_error',
                            'agent_type': self.get_agent_type(),
                            'model': self.model,
                            'error_type': type(e).__name__,
                            'error_message': str(e),
                            'response_time_ms': response_time,
                            'attempt': attempt + 1,
                            'max_retries': max_retries,
                            'timestamp': time.time()
                        }}
                    )

                    if isinstance(e, ValueError):
                        raise  # Re-raise ValueError as-is
                    else:
                        raise ConnectionError(f"Failed to get response from Ollama after {max_retries} attempts: {e}")

            except Exception as e:
                response_time = (time.time() - start_time) * 1000
                agent_logger.error(f"Unexpected error in Ollama request after {response_time:.2f}ms: {e}", extra={'structured_data': {
                    'event_type': 'agent_ollama_error',
                    'agent_type': self.get_agent_type(),
                    'model': self.model,
                    'error_type': type(e).__name__,
                    'error_message': str(e),
                    'response_time_ms': response_time,
                    'attempt': attempt + 1,
                    'timestamp': time.time()
                }})
                raise

        # Should never reach here, but just in case
        if last_exception:
            raise last_exception
        raise ConnectionError(f"Failed to get response from Ollama after {max_retries} attempts")
    
    def _generate_from_prompt(self, prompt: str, max_retries: int = 3, retry_delay: float = 1.0, **ollama_options) -> str:
        """
        Simple generation from a single prompt string using ollama.generate() with retry logic.

        Uses the ollama library's native generate() method for simple prompt-based generation.
        Useful for agents that don't need complex chat conversations.

        Args:
            prompt: The prompt string
            max_retries: Maximum number of retry attempts for transient failures (default: 3)
            retry_delay: Initial delay between retries in seconds (default: 1.0, doubles each retry)
            **ollama_options: Additional Ollama options (overrides defaults)

        Returns:
            The model's response content

        Raises:
            ConnectionError: If unable to connect to Ollama after retries
            ValueError: If the response is invalid after retries

        Examples:
            >>> response = self._generate_from_prompt("What is 2+2?")
            >>> print(response)
            "4"
        """
        agent_logger = logging.getLogger('bmlibrarian.agents')

        last_exception = None
        current_delay = retry_delay

        for attempt in range(max_retries):
            start_time = time.time()

            # Log the request (include attempt number if retrying)
            log_msg = f"Ollama generate request to {self.model}"
            if attempt > 0:
                log_msg += f" (retry {attempt}/{max_retries - 1})"

            agent_logger.info(log_msg, extra={'structured_data': {
                'event_type': 'agent_ollama_generate',
                'agent_type': self.get_agent_type(),
                'model': self.model,
                'prompt_length': len(prompt),
                'options': self._get_ollama_options(**ollama_options),
                'attempt': attempt + 1,
                'max_retries': max_retries,
                'timestamp': start_time
            }})

            try:
                # Get options with any overrides
                options = self._get_ollama_options(**ollama_options)

                response = self.client.generate(
                    model=self.model,
                    prompt=prompt,
                    options=options
                )

                content = response['response']
                if not content or not content.strip():
                    # Empty response is a transient failure - retry
                    raise ValueError("Empty response from model")

                # Log successful response
                response_time = (time.time() - start_time) * 1000
                wall_time_seconds = response_time / 1000.0
                success_msg = f"Ollama response received in {response_time:.2f}ms"
                if attempt > 0:
                    success_msg += f" (succeeded on retry {attempt})"

                # Extract token counts and timing from response
                prompt_tokens = response.get('prompt_eval_count', 0)
                completion_tokens = response.get('eval_count', 0)
                model_time_ns = response.get('eval_duration', 0)
                prompt_eval_ns = response.get('prompt_eval_duration', 0)

                # Track performance metrics
                self._metrics.add_request_metrics(
                    prompt_tokens=prompt_tokens,
                    completion_tokens=completion_tokens,
                    wall_time_seconds=wall_time_seconds,
                    model_time_ns=model_time_ns,
                    prompt_eval_ns=prompt_eval_ns,
                    retries=attempt
                )

                agent_logger.info(success_msg, extra={'structured_data': {
                    'event_type': 'agent_ollama_response',
                    'agent_type': self.get_agent_type(),
                    'model': self.model,
                    'response_length': len(content),
                    'response_time_ms': response_time,
                    'prompt_tokens': prompt_tokens,
                    'completion_tokens': completion_tokens,
                    'attempt': attempt + 1,
                    'timestamp': time.time()
                }})

                return content.strip()

            except (ollama.ResponseError, ValueError) as e:
                response_time = (time.time() - start_time) * 1000
                last_exception = e

                # Determine if we should retry
                is_retryable = isinstance(e, ValueError) or 'timeout' in str(e).lower() or 'connection' in str(e).lower()

                if attempt < max_retries - 1 and is_retryable:
                    # Transient failure - will retry
                    agent_logger.warning(
                        f"Transient error in generate request after {response_time:.2f}ms, retrying in {current_delay:.1f}s: {e}",
                        extra={'structured_data': {
                            'event_type': 'agent_ollama_retry',
                            'agent_type': self.get_agent_type(),
                            'model': self.model,
                            'error_type': type(e).__name__,
                            'error_message': str(e),
                            'response_time_ms': response_time,
                            'attempt': attempt + 1,
                            'retry_delay_s': current_delay,
                            'timestamp': time.time()
                        }}
                    )

                    # Wait before retrying (exponential backoff)
                    time.sleep(current_delay)
                    current_delay *= 2

                else:
                    # Final attempt failed or non-retryable error
                    agent_logger.error(
                        f"Generate request failed after {response_time:.2f}ms (attempt {attempt + 1}/{max_retries}): {e}",
                        extra={'structured_data': {
                            'event_type': 'agent_ollama_error',
                            'agent_type': self.get_agent_type(),
                            'model': self.model,
                            'error_type': type(e).__name__,
                            'error_message': str(e),
                            'response_time_ms': response_time,
                            'attempt': attempt + 1,
                            'max_retries': max_retries,
                            'timestamp': time.time()
                        }}
                    )

                    if isinstance(e, ValueError):
                        raise
                    else:
                        raise ConnectionError(f"Failed to get response from Ollama after {max_retries} attempts: {e}")

            except Exception as e:
                response_time = (time.time() - start_time) * 1000
                agent_logger.error(f"Unexpected error after {response_time:.2f}ms: {e}", extra={'structured_data': {
                    'event_type': 'agent_ollama_error',
                    'agent_type': self.get_agent_type(),
                    'model': self.model,
                    'error_type': type(e).__name__,
                    'error_message': str(e),
                    'response_time_ms': response_time,
                    'attempt': attempt + 1,
                    'timestamp': time.time()
                }})
                raise

        # Should never reach here, but just in case
        if last_exception:
            raise last_exception
        raise ConnectionError(f"Failed to get response from Ollama after {max_retries} attempts")

    def _generate_embedding(self, text: str, model: Optional[str] = None) -> list[float]:
        """
        Generate embedding vector for text using Ollama.

        Uses the ollama library's native embeddings() method for generating
        vector embeddings. Useful for semantic search and similarity tasks.

        Args:
            text: The text to embed
            model: Optional embedding model name (defaults to snowflake-arctic-embed2:latest)

        Returns:
            List of floats representing the embedding vector

        Raises:
            ConnectionError: If unable to connect to Ollama
            ValueError: If the response is invalid

        Examples:
            >>> embedding = self._generate_embedding("What is cardiovascular disease?")
            >>> len(embedding)
            768  # or 1024 depending on model
        """
        start_time = time.time()
        agent_logger = logging.getLogger('bmlibrarian.agents')

        # Default to snowflake-arctic-embed2 (model_id=1 in database)
        embedding_model = model or "snowflake-arctic-embed2:latest"

        # Log the request
        agent_logger.info(f"Ollama embedding request to {embedding_model}", extra={'structured_data': {
            'event_type': 'agent_ollama_embedding',
            'agent_type': self.get_agent_type(),
            'model': embedding_model,
            'text_length': len(text),
            'timestamp': start_time
        }})

        try:
            response = self.client.embeddings(
                model=embedding_model,
                prompt=text
            )

            embedding = response['embedding']
            if not embedding:
                raise ValueError("Empty embedding from model")

            # Log successful response
            response_time = (time.time() - start_time) * 1000
            agent_logger.info(f"Ollama embedding received in {response_time:.2f}ms", extra={'structured_data': {
                'event_type': 'agent_ollama_embedding_response',
                'agent_type': self.get_agent_type(),
                'model': embedding_model,
                'embedding_dim': len(embedding),
                'response_time_ms': response_time,
                'timestamp': time.time()
            }})

            return embedding

        except ollama.ResponseError as e:
            response_time = (time.time() - start_time) * 1000
            agent_logger.error(f"Ollama embedding error after {response_time:.2f}ms: {e}", extra={'structured_data': {
                'event_type': 'agent_ollama_error',
                'agent_type': self.get_agent_type(),
                'model': embedding_model,
                'error_type': 'ResponseError',
                'error_message': str(e),
                'response_time_ms': response_time,
                'timestamp': time.time()
            }})
            raise ConnectionError(f"Failed to get embedding from Ollama: {e}")
        except Exception as e:
            response_time = (time.time() - start_time) * 1000
            agent_logger.error(f"Unexpected embedding error after {response_time:.2f}ms: {e}", extra={'structured_data': {
                'event_type': 'agent_ollama_error',
                'agent_type': self.get_agent_type(),
                'model': embedding_model,
                'error_type': type(e).__name__,
                'error_message': str(e),
                'response_time_ms': response_time,
                'timestamp': time.time()
            }})
            raise

    def test_connection(self) -> bool:
        """
        Test the connection to Ollama server and verify model availability.

        Returns:
            True if connection is successful and model is available
        """
        try:
            models = self.client.list()
            available_models = [model.model for model in models.models]

            if self.model not in available_models:
                logger.warning(f"Model {self.model} not found. Available models: {available_models}")
                return False

            logger.info(f"Successfully connected to Ollama. Model {self.model} is available.")
            return True

        except Exception as e:
            logger.error(f"Failed to connect to Ollama: {e}")
            return False
    
    def get_available_models(self) -> list[str]:
        """
        Get list of available models from Ollama.
        
        Returns:
            List of available model names
            
        Raises:
            ConnectionError: If unable to connect to Ollama
        """
        try:
            models = self.client.list()
            return [model.model for model in models.models]
        except Exception as e:
            logger.error(f"Failed to get available models: {e}")
            raise ConnectionError(f"Failed to connect to Ollama: {e}")
    
    def set_callback(self, callback: Optional[Callable[[str, str], None]]) -> None:
        """
        Set or update the callback function.
        
        Args:
            callback: New callback function or None to disable callbacks
        """
        self.callback = callback
    
    def set_orchestrator(self, orchestrator: Optional["AgentOrchestrator"]) -> None:
        """
        Set or update the orchestrator for queue-based processing.
        
        Args:
            orchestrator: Orchestrator instance or None to disable queue processing
        """
        self.orchestrator = orchestrator
    
    def submit_task(self,
                   method_name: str,
                   data: Dict[str, Any],
                   target_agent: Optional[str] = None,
                   priority: Optional[Any] = None) -> Optional[str]:
        """
        Submit a task to the orchestrator queue.
        
        Args:
            method_name: Method to call on the target agent
            data: Task data/parameters
            target_agent: Target agent type (defaults to self)
            priority: Task priority (uses orchestrator default if None)
            
        Returns:
            Task ID if orchestrator is available, None otherwise
        """
        if not self.orchestrator:
            return None
        
        from .queue_manager import TaskPriority
        
        return self.orchestrator.submit_task(
            target_agent=target_agent or self.get_agent_type(),
            method_name=method_name,
            data=data,
            source_agent=self.get_agent_type(),
            priority=priority or TaskPriority.NORMAL
        )
    
    def submit_batch_tasks(self,
                          method_name: str,
                          data_list: list[Dict[str, Any]],
                          target_agent: Optional[str] = None,
                          priority: Optional[Any] = None) -> Optional[list[str]]:
        """
        Submit multiple tasks to the orchestrator queue.
        
        Args:
            method_name: Method to call on the target agent
            data_list: List of task data dictionaries
            target_agent: Target agent type (defaults to self)
            priority: Task priority (uses orchestrator default if None)
            
        Returns:
            List of task IDs if orchestrator is available, None otherwise
        """
        if not self.orchestrator:
            return None
        
        from .queue_manager import TaskPriority
        
        return self.orchestrator.submit_batch_tasks(
            target_agent=target_agent or self.get_agent_type(),
            method_name=method_name,
            data_list=data_list,
            source_agent=self.get_agent_type(),
            priority=priority or TaskPriority.NORMAL
        )

    def _parse_json_response(self, response: str) -> Dict:
        """
        Parse JSON response with robust error handling.

        Attempts to fix common JSON formatting issues before parsing.
        This method is available to all agents inheriting from BaseAgent.

        Args:
            response: Raw response string from model

        Returns:
            Parsed JSON dictionary

        Raises:
            json.JSONDecodeError: If JSON cannot be parsed
        """
        response = response.strip()

        # Remove markdown code block wrapper if present
        if response.startswith('```json') and response.endswith('```'):
            response = response[7:-3].strip()  # Remove ```json and ```
        elif response.startswith('```') and response.endswith('```'):
            response = response[3:-3].strip()   # Remove ``` and ```

        # Try parsing as-is first
        try:
            return json.loads(response)
        except json.JSONDecodeError:
            pass

        # Try to extract JSON from response if it has extra text
        json_start = response.find('{')
        json_end = response.rfind('}')

        if json_start != -1 and json_end != -1 and json_end > json_start:
            json_part = response[json_start:json_end + 1]
            try:
                return json.loads(json_part)
            except json.JSONDecodeError:
                pass

        # Try to fix incomplete JSON by adding missing closing braces/quotes
        if response.startswith('{') and not response.endswith('}'):
            # Try to complete the JSON structure
            attempts = [
                response + '"}',  # Missing closing quote and brace
                response + '}',   # Missing closing brace only
                response + '"}'   # Missing quote then brace
            ]

            for attempt in attempts:
                try:
                    return json.loads(attempt)
                except json.JSONDecodeError:
                    continue

        # If all attempts fail, raise the original error
        raise json.JSONDecodeError("Could not parse JSON response", response, 0)

    def _generate_and_parse_json(
        self,
        prompt: str,
        max_retries: int = 3,
        retry_context: str = "LLM generation",
        **ollama_options
    ) -> Dict:
        """
        Generate LLM response and parse as JSON with automatic retry on parse failures.

        When JSON parsing fails, regenerates the response (the LLM likely didn't follow
        instructions). This is more effective than retrying parse on the same bad response.

        Args:
            prompt: The prompt string
            max_retries: Maximum number of retry attempts (default: 3)
            retry_context: Description for logging (e.g., "citation extraction", "evaluation")
            **ollama_options: Additional Ollama options

        Returns:
            Parsed JSON dictionary

        Raises:
            json.JSONDecodeError: If JSON cannot be parsed after all retries
            ConnectionError: If unable to connect to Ollama

        Example:
            >>> result = self._generate_and_parse_json(
            ...     prompt="Return JSON with field 'answer'",
            ...     max_retries=3,
            ...     retry_context="question answering"
            ... )
        """
        agent_logger = logging.getLogger('bmlibrarian.agents')

        for attempt in range(max_retries + 1):
            try:
                # Log retry attempts (skip for first attempt)
                if attempt > 0:
                    agent_logger.info(
                        f"ðŸ”„ JSON PARSE RETRY {attempt}/{max_retries} for {retry_context} "
                        f"(previous parse failed)"
                    )

                # Generate response from LLM
                llm_response = self._generate_from_prompt(prompt, **ollama_options)

                # Try to parse as JSON
                try:
                    parsed = self._parse_json_response(llm_response)

                    # Success! Log if this was a retry
                    if attempt > 0:
                        agent_logger.info(
                            f"âœ… JSON PARSE RETRY SUCCESS for {retry_context}: "
                            f"Valid JSON received on attempt {attempt + 1}/{max_retries + 1}"
                        )

                    return parsed

                except json.JSONDecodeError as parse_error:
                    is_final_attempt = (attempt == max_retries)

                    if is_final_attempt:
                        # Final attempt failed - log and raise
                        agent_logger.error(
                            f"ðŸš« JSON PARSE FAILED for {retry_context}: "
                            f"All {max_retries + 1} attempts exhausted. "
                            f"Last response: {llm_response[:200]}..."
                        )
                        raise json.JSONDecodeError(
                            f"Could not parse JSON response after {max_retries + 1} attempts",
                            llm_response,
                            0
                        )
                    else:
                        # Not final attempt - log and retry with new generation
                        agent_logger.warning(
                            f"âš ï¸  JSON PARSE FAILED for {retry_context} (attempt {attempt + 1}): "
                            f"{str(parse_error)}. Will retry ({max_retries - attempt} attempts remaining)..."
                        )
                        continue  # Retry with new generation

            except (ConnectionError, ValueError) as e:
                # Don't retry on connection errors - these need user intervention
                agent_logger.error(f"LLM connection error during {retry_context}: {e}")
                raise

        # Should never reach here, but just in case
        raise json.JSONDecodeError(
            f"Could not parse JSON response for {retry_context}",
            "",
            0
        )

    def _chat_and_parse_json(
        self,
        messages: list,
        system_prompt: Optional[str] = None,
        max_retries: int = 3,
        retry_context: str = "LLM chat",
        **ollama_options
    ) -> Dict:
        """
        Send chat messages and parse response as JSON with automatic retry on parse failures.

        Similar to _generate_and_parse_json but uses chat-based API instead of simple generation.
        When JSON parsing fails, regenerates the response (the LLM likely didn't follow instructions).

        Args:
            messages: List of message dictionaries for the conversation
            system_prompt: Optional system prompt to prepend
            max_retries: Maximum number of retry attempts (default: 3)
            retry_context: Description for logging (e.g., "evaluation", "analysis")
            **ollama_options: Additional Ollama options

        Returns:
            Parsed JSON dictionary

        Raises:
            json.JSONDecodeError: If JSON cannot be parsed after all retries
            ConnectionError: If unable to connect to Ollama

        Example:
            >>> result = self._chat_and_parse_json(
            ...     messages=[{'role': 'user', 'content': 'Analyze this'}],
            ...     max_retries=3,
            ...     retry_context="statement evaluation"
            ... )
        """
        agent_logger = logging.getLogger('bmlibrarian.agents')

        for attempt in range(max_retries + 1):
            try:
                # Log retry attempts (skip for first attempt)
                if attempt > 0:
                    agent_logger.info(
                        f"ðŸ”„ JSON PARSE RETRY {attempt}/{max_retries} for {retry_context} "
                        f"(previous parse failed)"
                    )

                # Generate chat response from LLM
                llm_response = self._make_ollama_request(
                    messages=messages,
                    system_prompt=system_prompt,
                    **ollama_options
                )

                # Try to parse as JSON
                try:
                    parsed = self._parse_json_response(llm_response)

                    # Success! Log if this was a retry
                    if attempt > 0:
                        agent_logger.info(
                            f"âœ… JSON PARSE RETRY SUCCESS for {retry_context}: "
                            f"Valid JSON received on attempt {attempt + 1}/{max_retries + 1}"
                        )

                    return parsed

                except json.JSONDecodeError as parse_error:
                    is_final_attempt = (attempt == max_retries)

                    if is_final_attempt:
                        # Final attempt failed - log and raise
                        agent_logger.error(
                            f"ðŸš« JSON PARSE FAILED for {retry_context}: "
                            f"All {max_retries + 1} attempts exhausted. "
                            f"Last response: {llm_response[:200]}..."
                        )
                        raise json.JSONDecodeError(
                            f"Could not parse JSON response after {max_retries + 1} attempts",
                            llm_response,
                            0
                        )
                    else:
                        # Not final attempt - log and retry with new generation
                        agent_logger.warning(
                            f"âš ï¸  JSON PARSE FAILED for {retry_context} (attempt {attempt + 1}): "
                            f"{str(parse_error)}. Will retry ({max_retries - attempt} attempts remaining)..."
                        )
                        continue  # Retry with new generation

            except (ConnectionError, ValueError) as e:
                # Don't retry on connection errors - these need user intervention
                agent_logger.error(f"LLM connection error during {retry_context}: {e}")
                raise

        # Should never reach here, but just in case
        raise json.JSONDecodeError(
            f"Could not parse JSON response for {retry_context}",
            "",
            0
        )

    @abstractmethod
    def get_agent_type(self) -> str:
        """
        Get the type/name of this agent.

        Returns:
            String identifier for the agent type
        """
        pass

    # ============================================================================
    # Performance Metrics Methods
    # ============================================================================

    def get_performance_metrics(self) -> PerformanceMetrics:
        """
        Get the current performance metrics for this agent.

        Returns a copy of the metrics object to prevent external modification.
        Use this to access cumulative statistics about LLM usage.

        Returns:
            PerformanceMetrics object with current statistics

        Example:
            >>> agent = QueryAgent(model="gpt-oss:20b")
            >>> agent.search_documents("cardiovascular disease")
            >>> metrics = agent.get_performance_metrics()
            >>> print(f"Used {metrics.total_tokens} tokens")
        """
        return PerformanceMetrics(
            total_prompt_tokens=self._metrics.total_prompt_tokens,
            total_completion_tokens=self._metrics.total_completion_tokens,
            total_tokens=self._metrics.total_tokens,
            total_requests=self._metrics.total_requests,
            total_retries=self._metrics.total_retries,
            total_wall_time_seconds=self._metrics.total_wall_time_seconds,
            total_model_time_seconds=self._metrics.total_model_time_seconds,
            total_prompt_eval_seconds=self._metrics.total_prompt_eval_seconds,
            start_time=self._metrics.start_time,
            end_time=self._metrics.end_time
        )

    def reset_metrics(self) -> None:
        """
        Reset all performance metrics to initial values.

        Call this method to clear accumulated metrics, typically before
        starting a new task or operation that should be tracked separately.

        Example:
            >>> agent.reset_metrics()
            >>> agent.start_metrics()
            >>> # ... perform operations ...
            >>> agent.stop_metrics()
            >>> report = agent.format_metrics_report()
        """
        self._metrics.reset()

    def start_metrics(self) -> None:
        """
        Start tracking metrics for a new operation.

        Marks the start time for elapsed time calculation.
        Does not reset existing metrics - call reset_metrics() first if needed.

        Example:
            >>> agent.reset_metrics()
            >>> agent.start_metrics()
            >>> result = agent.process_document(doc)
            >>> agent.stop_metrics()
        """
        self._metrics.mark_start()

    def stop_metrics(self) -> None:
        """
        Stop tracking metrics for the current operation.

        Marks the end time for elapsed time calculation.

        Example:
            >>> agent.start_metrics()
            >>> result = agent.process_document(doc)
            >>> agent.stop_metrics()
            >>> print(f"Operation took {agent.get_performance_metrics().elapsed_time_seconds:.2f}s")
        """
        self._metrics.mark_end()

    def format_metrics_report(self, include_header: bool = True) -> str:
        """
        Generate a human-readable performance metrics report.

        Creates a formatted string report suitable for display to users
        or logging. Includes all key metrics with appropriate units.

        Args:
            include_header: Whether to include the agent type header (default: True)

        Returns:
            Formatted string report of performance metrics

        Example:
            >>> report = agent.format_metrics_report()
            >>> print(report)
            === QueryAgent Performance Metrics ===
            Requests:     5 (2 retries)
            Tokens:       12,450 total (10,200 prompt + 2,250 completion)
            Time:         15.32s elapsed (12.45s model time)
            Speed:        180.7 tokens/sec
        """
        metrics = self._metrics
        lines = []

        if include_header:
            agent_type = self.get_agent_type() if hasattr(self, 'get_agent_type') else "Agent"
            lines.append(f"=== {agent_type} Performance Metrics ===")

        # Request statistics
        retry_str = f" ({metrics.total_retries} retries)" if metrics.total_retries > 0 else ""
        lines.append(f"Requests:     {metrics.total_requests:,}{retry_str}")

        # Token statistics
        lines.append(
            f"Tokens:       {metrics.total_tokens:,} total "
            f"({metrics.total_prompt_tokens:,} prompt + "
            f"{metrics.total_completion_tokens:,} completion)"
        )

        # Timing statistics
        elapsed = metrics.elapsed_time_seconds
        model_time = metrics.total_model_time_seconds
        if elapsed > 0:
            lines.append(f"Time:         {elapsed:.2f}s elapsed ({model_time:.2f}s model time)")
        else:
            lines.append(f"Time:         {metrics.total_wall_time_seconds:.2f}s wall time ({model_time:.2f}s model time)")

        # Speed statistics
        if metrics.tokens_per_second > 0:
            lines.append(f"Speed:        {metrics.tokens_per_second:.1f} tokens/sec")

        # Average tokens per request
        if metrics.total_requests > 0:
            avg = metrics.average_tokens_per_request
            lines.append(f"Avg/Request:  {avg:.0f} tokens")

        return "\n".join(lines)

    def get_metrics_dict(self) -> Dict[str, Any]:
        """
        Get performance metrics as a dictionary for serialization.

        Useful for logging structured metrics, sending to monitoring systems,
        or storing in databases.

        Returns:
            Dictionary containing all metrics fields with agent metadata

        Example:
            >>> metrics_dict = agent.get_metrics_dict()
            >>> json.dumps(metrics_dict)  # For logging or storage
        """
        agent_type = self.get_agent_type() if hasattr(self, 'get_agent_type') else "Agent"
        return {
            'agent_type': agent_type,
            'model': self.model,
            **self._metrics.to_dict()
        }