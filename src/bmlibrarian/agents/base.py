"""
Base Agent Class for BMLibrarian AI Agents

Provides common functionality for all AI agents including:
- LLM abstraction layer integration (Ollama, Anthropic, OpenAI)
- Model configuration with provider selection
- Callback system for progress updates
- Error handling patterns
- Connection testing utilities
- Queue integration for large-scale processing
- Performance metrics tracking (execution time, token usage)
"""

import json
import logging
import time
from dataclasses import dataclass, field
from typing import Optional, Callable, Dict, Any, TYPE_CHECKING
from abc import ABC, abstractmethod

from ..llm import (
    LLMClient,
    LLMMessage,
    LLMResponse,
    Provider,
    parse_model_string,
    DEFAULT_TEMPERATURE,
    DEFAULT_TOP_P,
    DEFAULT_MAX_RETRIES,
    DEFAULT_RETRY_DELAY,
    DEFAULT_OLLAMA_HOST,
)


# Constants for nanosecond to second conversion
NANOSECONDS_PER_SECOND = 1_000_000_000


@dataclass
class PerformanceMetrics:
    """
    Performance metrics for tracking agent execution statistics.

    Tracks cumulative metrics across all LLM calls made by an agent,
    including token counts, timing information, and request statistics.

    Attributes:
        total_prompt_tokens: Total number of tokens sent to the model (input)
        total_completion_tokens: Total number of tokens generated by the model (output)
        total_tokens: Combined total of prompt and completion tokens
        total_requests: Number of successful LLM requests made
        total_retries: Number of retry attempts across all requests
        total_wall_time_seconds: Wall clock time for all operations (seconds)
        total_model_time_seconds: Time spent in model inference (seconds)
        total_prompt_eval_seconds: Time spent processing prompts (seconds)
        start_time: Timestamp when metrics collection started
        end_time: Timestamp when metrics collection ended (if completed)
    """
    total_prompt_tokens: int = 0
    total_completion_tokens: int = 0
    total_tokens: int = 0
    total_requests: int = 0
    total_retries: int = 0
    total_wall_time_seconds: float = 0.0
    total_model_time_seconds: float = 0.0
    total_prompt_eval_seconds: float = 0.0
    start_time: Optional[float] = None
    end_time: Optional[float] = None

    def add_request_metrics(
        self,
        prompt_tokens: int,
        completion_tokens: int,
        wall_time_seconds: float,
        model_time_ns: int = 0,
        prompt_eval_ns: int = 0,
        retries: int = 0
    ) -> None:
        """
        Add metrics from a single LLM request.

        Args:
            prompt_tokens: Number of tokens in the prompt (input)
            completion_tokens: Number of tokens in the response (output)
            wall_time_seconds: Wall clock time for the request (seconds)
            model_time_ns: Total model inference time in nanoseconds
            prompt_eval_ns: Prompt evaluation time in nanoseconds
            retries: Number of retry attempts for this request
        """
        self.total_prompt_tokens += prompt_tokens
        self.total_completion_tokens += completion_tokens
        self.total_tokens += (prompt_tokens + completion_tokens)
        self.total_requests += 1
        self.total_retries += retries
        self.total_wall_time_seconds += wall_time_seconds
        self.total_model_time_seconds += model_time_ns / NANOSECONDS_PER_SECOND
        self.total_prompt_eval_seconds += prompt_eval_ns / NANOSECONDS_PER_SECOND

    def mark_start(self) -> None:
        """Mark the start of a metrics collection period."""
        self.start_time = time.time()
        self.end_time = None

    def mark_end(self) -> None:
        """Mark the end of a metrics collection period."""
        self.end_time = time.time()

    @property
    def elapsed_time_seconds(self) -> float:
        """Get elapsed time from start to end (or current time if not ended)."""
        if self.start_time is None:
            return 0.0
        end = self.end_time if self.end_time is not None else time.time()
        return end - self.start_time

    @property
    def tokens_per_second(self) -> float:
        """Calculate average token generation rate (completion tokens per second)."""
        if self.total_model_time_seconds > 0:
            return self.total_completion_tokens / self.total_model_time_seconds
        return 0.0

    @property
    def average_tokens_per_request(self) -> float:
        """Calculate average tokens per request."""
        if self.total_requests > 0:
            return self.total_tokens / self.total_requests
        return 0.0

    def to_dict(self) -> Dict[str, Any]:
        """
        Convert metrics to a dictionary for serialization or logging.

        Returns:
            Dictionary containing all metrics fields
        """
        return {
            'total_prompt_tokens': self.total_prompt_tokens,
            'total_completion_tokens': self.total_completion_tokens,
            'total_tokens': self.total_tokens,
            'total_requests': self.total_requests,
            'total_retries': self.total_retries,
            'total_wall_time_seconds': round(self.total_wall_time_seconds, 3),
            'total_model_time_seconds': round(self.total_model_time_seconds, 3),
            'total_prompt_eval_seconds': round(self.total_prompt_eval_seconds, 3),
            'elapsed_time_seconds': round(self.elapsed_time_seconds, 3),
            'tokens_per_second': round(self.tokens_per_second, 2),
            'average_tokens_per_request': round(self.average_tokens_per_request, 1),
            'start_time': self.start_time,
            'end_time': self.end_time
        }

    def reset(self) -> None:
        """Reset all metrics to initial values."""
        self.total_prompt_tokens = 0
        self.total_completion_tokens = 0
        self.total_tokens = 0
        self.total_requests = 0
        self.total_retries = 0
        self.total_wall_time_seconds = 0.0
        self.total_model_time_seconds = 0.0
        self.total_prompt_eval_seconds = 0.0
        self.start_time = None
        self.end_time = None

if TYPE_CHECKING:
    from .orchestrator import AgentOrchestrator


logger = logging.getLogger(__name__)


class BaseAgent(ABC):
    """
    Base class for all BMLibrarian AI agents.

    Provides common functionality including LLM abstraction layer integration,
    callback management, and standardized error handling.

    Supports multiple LLM providers via model string prefixes:
        - "model-name" - Uses default provider (Ollama)
        - "ollama:model-name" - Explicit Ollama
        - "anthropic:claude-3-opus" - Anthropic Claude
        - "openai:gpt-4" - OpenAI (future)
    """

    def __init__(
        self,
        model: str,
        host: str = DEFAULT_OLLAMA_HOST,
        temperature: float = DEFAULT_TEMPERATURE,
        top_p: float = DEFAULT_TOP_P,
        callback: Optional[Callable[[str, str], None]] = None,
        orchestrator: Optional["AgentOrchestrator"] = None,
        show_model_info: bool = True,
        fallback_model: Optional[str] = None,
    ):
        """
        Initialize the base agent.

        Args:
            model: Model name with optional provider prefix (e.g., "anthropic:claude-3-opus")
            host: Ollama server host URL (default: http://localhost:11434)
            temperature: Model temperature for response randomness (0.0-1.0)
            top_p: Model top-p sampling parameter (0.0-1.0)
            callback: Optional callback function called with (step, data) for progress updates
            orchestrator: Optional orchestrator for queue-based processing
            show_model_info: Whether to display model information on initialization
            fallback_model: Optional Ollama model to use if primary provider fails
        """
        self.model = model
        self.host = host
        self.temperature = temperature
        self.top_p = top_p
        self.callback = callback
        self.fallback_model = fallback_model
        self.orchestrator = orchestrator

        # Parse model string to determine provider
        self._model_spec = parse_model_string(model)

        # Initialize LLM client (abstracts multiple providers)
        self._llm_client = LLMClient(
            default_provider=Provider.OLLAMA,
            fallback_provider=Provider.OLLAMA,
            fallback_model=fallback_model,
            track_usage=True,
            ollama_host=host,
        )

        # Initialize performance metrics tracking
        self._metrics = PerformanceMetrics()

        # Display model information if requested
        if show_model_info:
            self._display_model_info()

    @property
    def client(self) -> LLMClient:
        """
        Get the LLM client instance.

        Provided for backward compatibility. New code should use _llm_client directly.

        Returns:
            LLMClient instance
        """
        return self._llm_client

    def _display_model_info(self) -> None:
        """Display model information to terminal."""
        agent_type = self.get_agent_type() if hasattr(self, 'get_agent_type') else "Agent"
        print(f"ðŸ¤– {agent_type} initialized with model: {self.model}")
    
    def _call_callback(self, step: str, data: str) -> None:
        """
        Call the callback function if provided.
        
        Args:
            step: The current processing step
            data: Data associated with the step
        """
        if self.callback:
            try:
                self.callback(step, data)
            except Exception as e:
                logger.warning(f"Callback function failed for step '{step}': {e}")
    
    def _get_ollama_options(self, **overrides) -> Dict[str, Any]:
        """
        Get Ollama options with defaults and overrides.
        
        Args:
            **overrides: Option overrides to apply
            
        Returns:
            Dictionary of Ollama options
        """
        options = {
            'temperature': self.temperature,
            'top_p': self.top_p,
            'num_predict': getattr(self, 'max_tokens', 1000)  # Use config max_tokens or reasonable default
        }
        options.update(overrides)
        return options
    
    def _make_llm_request(
        self,
        messages: list,
        system_prompt: Optional[str] = None,
        max_retries: int = DEFAULT_MAX_RETRIES,
        retry_delay: float = DEFAULT_RETRY_DELAY,
        **llm_options
    ) -> str:
        """
        Make a request to the LLM with standardized error handling.

        Uses the LLM abstraction layer to support multiple providers.
        The provider is determined by the model string prefix.

        Args:
            messages: List of message dictionaries for the conversation
            system_prompt: Optional system prompt to prepend
            max_retries: Maximum number of retry attempts for transient failures
            retry_delay: Initial delay between retries in seconds
            **llm_options: Additional LLM options (max_tokens, json_mode, etc.)

        Returns:
            The model's response content

        Raises:
            ConnectionError: If unable to connect to LLM after retries
            ValueError: If the response is invalid after retries
        """
        agent_logger = logging.getLogger('bmlibrarian.agents')
        start_time = time.time()

        # Log the request
        agent_logger.info(f"LLM request to {self.model}", extra={'structured_data': {
            'event_type': 'agent_llm_request',
            'agent_type': self.get_agent_type(),
            'model': self.model,
            'provider': self._model_spec.provider.value,
            'message_count': len(messages),
            'has_system_prompt': system_prompt is not None,
            'timestamp': start_time
        }})

        try:
            # Convert dict messages to LLMMessage objects
            llm_messages = [
                LLMMessage(role=msg['role'], content=msg['content'])
                for msg in messages
            ]

            # Extract supported options
            max_tokens = llm_options.pop('num_predict', None)
            json_mode = llm_options.pop('json_mode', False)

            # Make request via LLM client
            response: LLMResponse = self._llm_client.chat(
                messages=llm_messages,
                model=self.model,
                system_prompt=system_prompt,
                temperature=self.temperature,
                top_p=self.top_p,
                max_tokens=max_tokens,
                json_mode=json_mode,
                fallback_model=self.fallback_model,
                max_retries=max_retries,
                retry_delay=retry_delay,
            )

            content = response.content
            if not content or not content.strip():
                raise ValueError("Empty response from model")

            # Log successful response
            response_time = (time.time() - start_time) * 1000

            # Track performance metrics from response
            self._metrics.add_request_metrics(
                prompt_tokens=response.prompt_tokens,
                completion_tokens=response.completion_tokens,
                wall_time_seconds=response.duration_seconds,
                model_time_ns=0,  # Not available from LLMResponse
                prompt_eval_ns=0,  # Not available from LLMResponse
                retries=0
            )

            agent_logger.info(f"LLM response received in {response_time:.2f}ms", extra={'structured_data': {
                'event_type': 'agent_llm_response',
                'agent_type': self.get_agent_type(),
                'model': response.model,
                'provider': response.provider.value,
                'response_length': len(content),
                'response_time_ms': response_time,
                'prompt_tokens': response.prompt_tokens,
                'completion_tokens': response.completion_tokens,
                'timestamp': time.time()
            }})

            return content.strip()

        except ConnectionError as e:
            response_time = (time.time() - start_time) * 1000
            agent_logger.error(f"LLM request failed after {response_time:.2f}ms: {e}", extra={'structured_data': {
                'event_type': 'agent_llm_error',
                'agent_type': self.get_agent_type(),
                'model': self.model,
                'provider': self._model_spec.provider.value,
                'error_type': type(e).__name__,
                'error_message': str(e),
                'response_time_ms': response_time,
                'timestamp': time.time()
            }})
            raise

        except Exception as e:
            response_time = (time.time() - start_time) * 1000
            agent_logger.error(f"Unexpected error in LLM request after {response_time:.2f}ms: {e}", extra={'structured_data': {
                'event_type': 'agent_llm_error',
                'agent_type': self.get_agent_type(),
                'model': self.model,
                'provider': self._model_spec.provider.value,
                'error_type': type(e).__name__,
                'error_message': str(e),
                'response_time_ms': response_time,
                'timestamp': time.time()
            }})
            raise

    # Backward compatibility alias
    def _make_ollama_request(
        self,
        messages: list,
        system_prompt: Optional[str] = None,
        max_retries: int = DEFAULT_MAX_RETRIES,
        retry_delay: float = DEFAULT_RETRY_DELAY,
        **ollama_options
    ) -> str:
        """
        Backward compatibility wrapper for _make_llm_request.

        Deprecated: Use _make_llm_request instead.
        """
        return self._make_llm_request(
            messages=messages,
            system_prompt=system_prompt,
            max_retries=max_retries,
            retry_delay=retry_delay,
            **ollama_options
        )
    
    def _generate_from_prompt(
        self,
        prompt: str,
        max_retries: int = DEFAULT_MAX_RETRIES,
        retry_delay: float = DEFAULT_RETRY_DELAY,
        **llm_options
    ) -> str:
        """
        Simple generation from a single prompt string.

        Uses the LLM abstraction layer for text generation.
        Useful for agents that don't need complex chat conversations.

        Args:
            prompt: The prompt string
            max_retries: Maximum number of retry attempts for transient failures
            retry_delay: Initial delay between retries in seconds
            **llm_options: Additional LLM options (max_tokens, json_mode, etc.)

        Returns:
            The model's response content

        Raises:
            ConnectionError: If unable to connect to LLM after retries
            ValueError: If the response is invalid after retries

        Examples:
            >>> response = self._generate_from_prompt("What is 2+2?")
            >>> print(response)
            "4"
        """
        agent_logger = logging.getLogger('bmlibrarian.agents')
        start_time = time.time()

        # Log the request
        agent_logger.info(f"LLM generate request to {self.model}", extra={'structured_data': {
            'event_type': 'agent_llm_generate',
            'agent_type': self.get_agent_type(),
            'model': self.model,
            'provider': self._model_spec.provider.value,
            'prompt_length': len(prompt),
            'timestamp': start_time
        }})

        try:
            # Extract supported options
            max_tokens = llm_options.pop('num_predict', None)
            json_mode = llm_options.pop('json_mode', False)

            # Make request via LLM client
            response: LLMResponse = self._llm_client.generate(
                prompt=prompt,
                model=self.model,
                temperature=self.temperature,
                top_p=self.top_p,
                max_tokens=max_tokens,
                json_mode=json_mode,
                fallback_model=self.fallback_model,
                max_retries=max_retries,
                retry_delay=retry_delay,
            )

            content = response.content
            if not content or not content.strip():
                raise ValueError("Empty response from model")

            # Log successful response
            response_time = (time.time() - start_time) * 1000

            # Track performance metrics from response
            self._metrics.add_request_metrics(
                prompt_tokens=response.prompt_tokens,
                completion_tokens=response.completion_tokens,
                wall_time_seconds=response.duration_seconds,
                model_time_ns=0,
                prompt_eval_ns=0,
                retries=0
            )

            agent_logger.info(f"LLM response received in {response_time:.2f}ms", extra={'structured_data': {
                'event_type': 'agent_llm_response',
                'agent_type': self.get_agent_type(),
                'model': response.model,
                'provider': response.provider.value,
                'response_length': len(content),
                'response_time_ms': response_time,
                'prompt_tokens': response.prompt_tokens,
                'completion_tokens': response.completion_tokens,
                'timestamp': time.time()
            }})

            return content.strip()

        except ConnectionError as e:
            response_time = (time.time() - start_time) * 1000
            agent_logger.error(f"LLM generate failed after {response_time:.2f}ms: {e}", extra={'structured_data': {
                'event_type': 'agent_llm_error',
                'agent_type': self.get_agent_type(),
                'model': self.model,
                'provider': self._model_spec.provider.value,
                'error_type': type(e).__name__,
                'error_message': str(e),
                'response_time_ms': response_time,
                'timestamp': time.time()
            }})
            raise

        except Exception as e:
            response_time = (time.time() - start_time) * 1000
            agent_logger.error(f"Unexpected error in LLM generate after {response_time:.2f}ms: {e}", extra={'structured_data': {
                'event_type': 'agent_llm_error',
                'agent_type': self.get_agent_type(),
                'model': self.model,
                'provider': self._model_spec.provider.value,
                'error_type': type(e).__name__,
                'error_message': str(e),
                'response_time_ms': response_time,
                'timestamp': time.time()
            }})
            raise

    def _generate_embedding(self, text: str, model: Optional[str] = None) -> list[float]:
        """
        Generate embedding vector for text.

        Uses the LLM abstraction layer for generating vector embeddings.
        Note: Embeddings always use Ollama for consistency with pgvector dimensions.

        Args:
            text: The text to embed
            model: Optional embedding model name (defaults to snowflake-arctic-embed2:latest)

        Returns:
            List of floats representing the embedding vector

        Raises:
            ConnectionError: If unable to connect to Ollama
            ValueError: If the response is invalid

        Examples:
            >>> embedding = self._generate_embedding("What is cardiovascular disease?")
            >>> len(embedding)
            768  # or 1024 depending on model
        """
        from ..llm import DEFAULT_EMBEDDING_MODEL

        start_time = time.time()
        agent_logger = logging.getLogger('bmlibrarian.agents')

        # Default to configured embedding model
        embedding_model = model or DEFAULT_EMBEDDING_MODEL

        # Log the request
        agent_logger.info(f"LLM embedding request to {embedding_model}", extra={'structured_data': {
            'event_type': 'agent_llm_embedding',
            'agent_type': self.get_agent_type(),
            'model': embedding_model,
            'text_length': len(text),
            'timestamp': start_time
        }})

        try:
            response = self._llm_client.embed(text=text, model=embedding_model)

            embedding = response.embedding
            if not embedding:
                raise ValueError("Empty embedding from model")

            # Log successful response
            response_time = (time.time() - start_time) * 1000
            agent_logger.info(f"LLM embedding received in {response_time:.2f}ms", extra={'structured_data': {
                'event_type': 'agent_llm_embedding_response',
                'agent_type': self.get_agent_type(),
                'model': embedding_model,
                'embedding_dim': len(embedding),
                'response_time_ms': response_time,
                'timestamp': time.time()
            }})

            return embedding

        except ConnectionError as e:
            response_time = (time.time() - start_time) * 1000
            agent_logger.error(f"LLM embedding error after {response_time:.2f}ms: {e}", extra={'structured_data': {
                'event_type': 'agent_llm_error',
                'agent_type': self.get_agent_type(),
                'model': embedding_model,
                'error_type': 'ConnectionError',
                'error_message': str(e),
                'response_time_ms': response_time,
                'timestamp': time.time()
            }})
            raise
        except Exception as e:
            response_time = (time.time() - start_time) * 1000
            agent_logger.error(f"Unexpected embedding error after {response_time:.2f}ms: {e}", extra={'structured_data': {
                'event_type': 'agent_llm_error',
                'agent_type': self.get_agent_type(),
                'model': embedding_model,
                'error_type': type(e).__name__,
                'error_message': str(e),
                'response_time_ms': response_time,
                'timestamp': time.time()
            }})
            raise

    def test_connection(self) -> bool:
        """
        Test the connection to LLM provider and verify model availability.

        Returns:
            True if connection is successful and model is available
        """
        try:
            # Test the provider for the configured model
            if not self._llm_client.test_provider(self._model_spec.provider):
                logger.warning(f"Provider {self._model_spec.provider.value} is not available")
                return False

            # For Ollama, also check if the specific model is available
            if self._model_spec.provider == Provider.OLLAMA:
                models = self._llm_client.list_models(Provider.OLLAMA)
                available_models = models.get('ollama', [])

                if self._model_spec.model_name not in available_models:
                    logger.warning(
                        f"Model {self._model_spec.model_name} not found. "
                        f"Available models: {available_models}"
                    )
                    return False

            logger.info(
                f"Successfully connected to {self._model_spec.provider.value}. "
                f"Model {self._model_spec.model_name} is available."
            )
            return True

        except Exception as e:
            logger.error(f"Failed to connect to LLM provider: {e}")
            return False

    def get_available_models(self) -> list[str]:
        """
        Get list of available models from the configured provider.

        Returns:
            List of available model names

        Raises:
            ConnectionError: If unable to connect to provider
        """
        try:
            models = self._llm_client.list_models(self._model_spec.provider)
            return models.get(self._model_spec.provider.value, [])
        except Exception as e:
            logger.error(f"Failed to get available models: {e}")
            raise ConnectionError(f"Failed to connect to LLM provider: {e}")
    
    def set_callback(self, callback: Optional[Callable[[str, str], None]]) -> None:
        """
        Set or update the callback function.
        
        Args:
            callback: New callback function or None to disable callbacks
        """
        self.callback = callback
    
    def set_orchestrator(self, orchestrator: Optional["AgentOrchestrator"]) -> None:
        """
        Set or update the orchestrator for queue-based processing.
        
        Args:
            orchestrator: Orchestrator instance or None to disable queue processing
        """
        self.orchestrator = orchestrator
    
    def submit_task(self,
                   method_name: str,
                   data: Dict[str, Any],
                   target_agent: Optional[str] = None,
                   priority: Optional[Any] = None) -> Optional[str]:
        """
        Submit a task to the orchestrator queue.
        
        Args:
            method_name: Method to call on the target agent
            data: Task data/parameters
            target_agent: Target agent type (defaults to self)
            priority: Task priority (uses orchestrator default if None)
            
        Returns:
            Task ID if orchestrator is available, None otherwise
        """
        if not self.orchestrator:
            return None
        
        from .queue_manager import TaskPriority
        
        return self.orchestrator.submit_task(
            target_agent=target_agent or self.get_agent_type(),
            method_name=method_name,
            data=data,
            source_agent=self.get_agent_type(),
            priority=priority or TaskPriority.NORMAL
        )
    
    def submit_batch_tasks(self,
                          method_name: str,
                          data_list: list[Dict[str, Any]],
                          target_agent: Optional[str] = None,
                          priority: Optional[Any] = None) -> Optional[list[str]]:
        """
        Submit multiple tasks to the orchestrator queue.
        
        Args:
            method_name: Method to call on the target agent
            data_list: List of task data dictionaries
            target_agent: Target agent type (defaults to self)
            priority: Task priority (uses orchestrator default if None)
            
        Returns:
            List of task IDs if orchestrator is available, None otherwise
        """
        if not self.orchestrator:
            return None
        
        from .queue_manager import TaskPriority
        
        return self.orchestrator.submit_batch_tasks(
            target_agent=target_agent or self.get_agent_type(),
            method_name=method_name,
            data_list=data_list,
            source_agent=self.get_agent_type(),
            priority=priority or TaskPriority.NORMAL
        )

    def _parse_json_response(self, response: str) -> Dict:
        """
        Parse JSON response with robust error handling.

        Attempts to fix common JSON formatting issues before parsing.
        This method is available to all agents inheriting from BaseAgent.

        Args:
            response: Raw response string from model

        Returns:
            Parsed JSON dictionary

        Raises:
            json.JSONDecodeError: If JSON cannot be parsed
        """
        response = response.strip()

        # Remove markdown code block wrapper if present
        if response.startswith('```json') and response.endswith('```'):
            response = response[7:-3].strip()  # Remove ```json and ```
        elif response.startswith('```') and response.endswith('```'):
            response = response[3:-3].strip()   # Remove ``` and ```

        # Try parsing as-is first
        try:
            return json.loads(response)
        except json.JSONDecodeError:
            pass

        # Try to extract JSON from response if it has extra text
        json_start = response.find('{')
        json_end = response.rfind('}')

        if json_start != -1 and json_end != -1 and json_end > json_start:
            json_part = response[json_start:json_end + 1]
            try:
                return json.loads(json_part)
            except json.JSONDecodeError:
                pass

        # Try to fix incomplete JSON by adding missing closing braces/quotes
        if response.startswith('{') and not response.endswith('}'):
            # Try to complete the JSON structure
            attempts = [
                response + '"}',  # Missing closing quote and brace
                response + '}',   # Missing closing brace only
                response + '"}'   # Missing quote then brace
            ]

            for attempt in attempts:
                try:
                    return json.loads(attempt)
                except json.JSONDecodeError:
                    continue

        # If all attempts fail, raise the original error
        raise json.JSONDecodeError("Could not parse JSON response", response, 0)

    def _generate_and_parse_json(
        self,
        prompt: str,
        max_retries: int = 3,
        retry_context: str = "LLM generation",
        **ollama_options
    ) -> Dict:
        """
        Generate LLM response and parse as JSON with automatic retry on parse failures.

        When JSON parsing fails, regenerates the response (the LLM likely didn't follow
        instructions). This is more effective than retrying parse on the same bad response.

        Args:
            prompt: The prompt string
            max_retries: Maximum number of retry attempts (default: 3)
            retry_context: Description for logging (e.g., "citation extraction", "evaluation")
            **ollama_options: Additional Ollama options

        Returns:
            Parsed JSON dictionary

        Raises:
            json.JSONDecodeError: If JSON cannot be parsed after all retries
            ConnectionError: If unable to connect to Ollama

        Example:
            >>> result = self._generate_and_parse_json(
            ...     prompt="Return JSON with field 'answer'",
            ...     max_retries=3,
            ...     retry_context="question answering"
            ... )
        """
        agent_logger = logging.getLogger('bmlibrarian.agents')

        for attempt in range(max_retries + 1):
            try:
                # Log retry attempts (skip for first attempt)
                if attempt > 0:
                    agent_logger.info(
                        f"ðŸ”„ JSON PARSE RETRY {attempt}/{max_retries} for {retry_context} "
                        f"(previous parse failed)"
                    )

                # Generate response from LLM
                llm_response = self._generate_from_prompt(prompt, **ollama_options)

                # Try to parse as JSON
                try:
                    parsed = self._parse_json_response(llm_response)

                    # Success! Log if this was a retry
                    if attempt > 0:
                        agent_logger.info(
                            f"âœ… JSON PARSE RETRY SUCCESS for {retry_context}: "
                            f"Valid JSON received on attempt {attempt + 1}/{max_retries + 1}"
                        )

                    return parsed

                except json.JSONDecodeError as parse_error:
                    is_final_attempt = (attempt == max_retries)

                    if is_final_attempt:
                        # Final attempt failed - log and raise
                        agent_logger.error(
                            f"ðŸš« JSON PARSE FAILED for {retry_context}: "
                            f"All {max_retries + 1} attempts exhausted. "
                            f"Last response: {llm_response[:200]}..."
                        )
                        raise json.JSONDecodeError(
                            f"Could not parse JSON response after {max_retries + 1} attempts",
                            llm_response,
                            0
                        )
                    else:
                        # Not final attempt - log and retry with new generation
                        agent_logger.warning(
                            f"âš ï¸  JSON PARSE FAILED for {retry_context} (attempt {attempt + 1}): "
                            f"{str(parse_error)}. Will retry ({max_retries - attempt} attempts remaining)..."
                        )
                        continue  # Retry with new generation

            except ConnectionError as e:
                # Don't retry on connection errors - these need user intervention
                agent_logger.error(f"LLM connection error during {retry_context}: {e}")
                raise

            except ValueError as e:
                # Empty response is a transient error that can be retried
                is_final_attempt = (attempt == max_retries)
                if is_final_attempt:
                    agent_logger.error(
                        f"ðŸš« LLM ERROR for {retry_context}: "
                        f"All {max_retries + 1} attempts exhausted. Error: {e}"
                    )
                    raise
                else:
                    agent_logger.warning(
                        f"âš ï¸  LLM ERROR for {retry_context} (attempt {attempt + 1}): "
                        f"{e}. Will retry ({max_retries - attempt} attempts remaining)..."
                    )
                    continue  # Retry with new generation

        # Should never reach here, but just in case
        raise json.JSONDecodeError(
            f"Could not parse JSON response for {retry_context}",
            "",
            0
        )

    def _chat_and_parse_json(
        self,
        messages: list,
        system_prompt: Optional[str] = None,
        max_retries: int = 3,
        retry_context: str = "LLM chat",
        **ollama_options
    ) -> Dict:
        """
        Send chat messages and parse response as JSON with automatic retry on parse failures.

        Similar to _generate_and_parse_json but uses chat-based API instead of simple generation.
        When JSON parsing fails, regenerates the response (the LLM likely didn't follow instructions).

        Args:
            messages: List of message dictionaries for the conversation
            system_prompt: Optional system prompt to prepend
            max_retries: Maximum number of retry attempts (default: 3)
            retry_context: Description for logging (e.g., "evaluation", "analysis")
            **ollama_options: Additional Ollama options

        Returns:
            Parsed JSON dictionary

        Raises:
            json.JSONDecodeError: If JSON cannot be parsed after all retries
            ConnectionError: If unable to connect to Ollama

        Example:
            >>> result = self._chat_and_parse_json(
            ...     messages=[{'role': 'user', 'content': 'Analyze this'}],
            ...     max_retries=3,
            ...     retry_context="statement evaluation"
            ... )
        """
        agent_logger = logging.getLogger('bmlibrarian.agents')

        for attempt in range(max_retries + 1):
            try:
                # Log retry attempts (skip for first attempt)
                if attempt > 0:
                    agent_logger.info(
                        f"ðŸ”„ JSON PARSE RETRY {attempt}/{max_retries} for {retry_context} "
                        f"(previous parse failed)"
                    )

                # Generate chat response from LLM
                llm_response = self._make_ollama_request(
                    messages=messages,
                    system_prompt=system_prompt,
                    **ollama_options
                )

                # Try to parse as JSON
                try:
                    parsed = self._parse_json_response(llm_response)

                    # Success! Log if this was a retry
                    if attempt > 0:
                        agent_logger.info(
                            f"âœ… JSON PARSE RETRY SUCCESS for {retry_context}: "
                            f"Valid JSON received on attempt {attempt + 1}/{max_retries + 1}"
                        )

                    return parsed

                except json.JSONDecodeError as parse_error:
                    is_final_attempt = (attempt == max_retries)

                    if is_final_attempt:
                        # Final attempt failed - log and raise
                        agent_logger.error(
                            f"ðŸš« JSON PARSE FAILED for {retry_context}: "
                            f"All {max_retries + 1} attempts exhausted. "
                            f"Last response: {llm_response[:200]}..."
                        )
                        raise json.JSONDecodeError(
                            f"Could not parse JSON response after {max_retries + 1} attempts",
                            llm_response,
                            0
                        )
                    else:
                        # Not final attempt - log and retry with new generation
                        agent_logger.warning(
                            f"âš ï¸  JSON PARSE FAILED for {retry_context} (attempt {attempt + 1}): "
                            f"{str(parse_error)}. Will retry ({max_retries - attempt} attempts remaining)..."
                        )
                        continue  # Retry with new generation

            except ConnectionError as e:
                # Don't retry on connection errors - these need user intervention
                agent_logger.error(f"LLM connection error during {retry_context}: {e}")
                raise

            except ValueError as e:
                # Empty response is a transient error that can be retried
                is_final_attempt = (attempt == max_retries)
                if is_final_attempt:
                    agent_logger.error(
                        f"ðŸš« LLM ERROR for {retry_context}: "
                        f"All {max_retries + 1} attempts exhausted. Error: {e}"
                    )
                    raise
                else:
                    agent_logger.warning(
                        f"âš ï¸  LLM ERROR for {retry_context} (attempt {attempt + 1}): "
                        f"{e}. Will retry ({max_retries - attempt} attempts remaining)..."
                    )
                    continue  # Retry with new generation

        # Should never reach here, but just in case
        raise json.JSONDecodeError(
            f"Could not parse JSON response for {retry_context}",
            "",
            0
        )

    @abstractmethod
    def get_agent_type(self) -> str:
        """
        Get the type/name of this agent.

        Returns:
            String identifier for the agent type
        """
        pass

    # ============================================================================
    # Performance Metrics Methods
    # ============================================================================

    def get_performance_metrics(self) -> PerformanceMetrics:
        """
        Get the current performance metrics for this agent.

        Returns a copy of the metrics object to prevent external modification.
        Use this to access cumulative statistics about LLM usage.

        Returns:
            PerformanceMetrics object with current statistics

        Example:
            >>> agent = QueryAgent(model="gpt-oss:20b")
            >>> agent.search_documents("cardiovascular disease")
            >>> metrics = agent.get_performance_metrics()
            >>> print(f"Used {metrics.total_tokens} tokens")
        """
        return PerformanceMetrics(
            total_prompt_tokens=self._metrics.total_prompt_tokens,
            total_completion_tokens=self._metrics.total_completion_tokens,
            total_tokens=self._metrics.total_tokens,
            total_requests=self._metrics.total_requests,
            total_retries=self._metrics.total_retries,
            total_wall_time_seconds=self._metrics.total_wall_time_seconds,
            total_model_time_seconds=self._metrics.total_model_time_seconds,
            total_prompt_eval_seconds=self._metrics.total_prompt_eval_seconds,
            start_time=self._metrics.start_time,
            end_time=self._metrics.end_time
        )

    def reset_metrics(self) -> None:
        """
        Reset all performance metrics to initial values.

        Call this method to clear accumulated metrics, typically before
        starting a new task or operation that should be tracked separately.

        Example:
            >>> agent.reset_metrics()
            >>> agent.start_metrics()
            >>> # ... perform operations ...
            >>> agent.stop_metrics()
            >>> report = agent.format_metrics_report()
        """
        self._metrics.reset()

    def start_metrics(self) -> None:
        """
        Start tracking metrics for a new operation.

        Marks the start time for elapsed time calculation.
        Does not reset existing metrics - call reset_metrics() first if needed.

        Example:
            >>> agent.reset_metrics()
            >>> agent.start_metrics()
            >>> result = agent.process_document(doc)
            >>> agent.stop_metrics()
        """
        self._metrics.mark_start()

    def stop_metrics(self) -> None:
        """
        Stop tracking metrics for the current operation.

        Marks the end time for elapsed time calculation.

        Example:
            >>> agent.start_metrics()
            >>> result = agent.process_document(doc)
            >>> agent.stop_metrics()
            >>> print(f"Operation took {agent.get_performance_metrics().elapsed_time_seconds:.2f}s")
        """
        self._metrics.mark_end()

    def format_metrics_report(self, include_header: bool = True) -> str:
        """
        Generate a human-readable performance metrics report.

        Creates a formatted string report suitable for display to users
        or logging. Includes all key metrics with appropriate units.

        Args:
            include_header: Whether to include the agent type header (default: True)

        Returns:
            Formatted string report of performance metrics

        Example:
            >>> report = agent.format_metrics_report()
            >>> print(report)
            === QueryAgent Performance Metrics ===
            Requests:     5 (2 retries)
            Tokens:       12,450 total (10,200 prompt + 2,250 completion)
            Time:         15.32s elapsed (12.45s model time)
            Speed:        180.7 tokens/sec
        """
        metrics = self._metrics
        lines = []

        if include_header:
            agent_type = self.get_agent_type() if hasattr(self, 'get_agent_type') else "Agent"
            lines.append(f"=== {agent_type} Performance Metrics ===")

        # Request statistics
        retry_str = f" ({metrics.total_retries} retries)" if metrics.total_retries > 0 else ""
        lines.append(f"Requests:     {metrics.total_requests:,}{retry_str}")

        # Token statistics
        lines.append(
            f"Tokens:       {metrics.total_tokens:,} total "
            f"({metrics.total_prompt_tokens:,} prompt + "
            f"{metrics.total_completion_tokens:,} completion)"
        )

        # Timing statistics
        elapsed = metrics.elapsed_time_seconds
        model_time = metrics.total_model_time_seconds
        if elapsed > 0:
            lines.append(f"Time:         {elapsed:.2f}s elapsed ({model_time:.2f}s model time)")
        else:
            lines.append(f"Time:         {metrics.total_wall_time_seconds:.2f}s wall time ({model_time:.2f}s model time)")

        # Speed statistics
        if metrics.tokens_per_second > 0:
            lines.append(f"Speed:        {metrics.tokens_per_second:.1f} tokens/sec")

        # Average tokens per request
        if metrics.total_requests > 0:
            avg = metrics.average_tokens_per_request
            lines.append(f"Avg/Request:  {avg:.0f} tokens")

        return "\n".join(lines)

    def get_metrics_dict(self) -> Dict[str, Any]:
        """
        Get performance metrics as a dictionary for serialization.

        Useful for logging structured metrics, sending to monitoring systems,
        or storing in databases.

        Returns:
            Dictionary containing all metrics fields with agent metadata

        Example:
            >>> metrics_dict = agent.get_metrics_dict()
            >>> json.dumps(metrics_dict)  # For logging or storage
        """
        agent_type = self.get_agent_type() if hasattr(self, 'get_agent_type') else "Agent"
        return {
            'agent_type': agent_type,
            'model': self.model,
            **self._metrics.to_dict()
        }