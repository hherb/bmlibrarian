"""
Report Formatting Module

Handles markdown report generation, file export utilities, and citation formatting.
"""

import os
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Optional
from bmlibrarian.agents import Report, Citation, CounterfactualAnalysis, EditedReport


class ReportFormatter:
    """Handles report formatting and export operations."""
    
    def __init__(self, config, ui):
        self.config = config
        self.ui = ui
    
    def format_report_as_markdown(self, report: Report, counterfactual_analysis: Optional[CounterfactualAnalysis] = None) -> str:
        """Format report as markdown with proper structure."""
        lines = []
        
        # Title and metadata
        lines.append(f"# Medical Literature Research Report")
        lines.append("")
        lines.append(f"**Generated by BMLibrarian CLI**  ")
        lines.append(f"**Date:** {report.created_at.strftime('%Y-%m-%d %H:%M:%S UTC')}  ")
        lines.append(f"**Evidence Strength:** {report.evidence_strength}  ")
        lines.append("")
        
        # Research question
        lines.append("## Research Question")
        lines.append("")
        lines.append(f"> {report.user_question}")
        lines.append("")
        
        # Evidence assessment
        lines.append("## Evidence Assessment")
        lines.append("")
        lines.append(f"- **Evidence Strength:** {report.evidence_strength}")
        lines.append(f"- **Citations Analyzed:** {report.citation_count}")
        lines.append(f"- **Unique References:** {report.unique_documents}")
        lines.append("")
        
        # Synthesized answer
        lines.append("## Findings")
        lines.append("")
        lines.append(report.synthesized_answer)
        lines.append("")
        
        # References
        lines.append("## References")
        lines.append("")
        for ref in report.references:
            formatted_ref = ref.format_vancouver_style()
            lines.append(f"{ref.number}. {formatted_ref}")
        lines.append("")
        
        # Methodology
        if report.methodology_note:
            lines.append("## Methodology")
            lines.append("")
            lines.append(report.methodology_note)
            lines.append("")
        
        # Add counterfactual analysis section if available
        if counterfactual_analysis:
            lines.extend(self._get_counterfactual_analysis_section(counterfactual_analysis))
        
        # Technical details
        lines.extend(self._get_technical_details_section())
        
        return "\n".join(lines)
    
    def _get_technical_details_section(self) -> List[str]:
        """Get technical details section for the report."""
        lines = []
        
        lines.append("## Technical Details")
        lines.append("")
        lines.append("This report was generated using the BMLibrarian multi-agent system:")
        lines.append("")
        lines.append("1. **Query Generation:** Natural language question converted to database query")
        lines.append("2. **Document Retrieval:** PostgreSQL full-text search with pgvector extension")
        lines.append("3. **Relevance Scoring:** AI-powered document scoring (1-5 scale)")
        lines.append("4. **Citation Extraction:** Relevant passage extraction from high-scoring documents")
        lines.append("5. **Report Synthesis:** Medical publication-style report generation")
        lines.append("")
        lines.append("**AI Models Used:**")
        lines.append("- Document scoring and citation extraction: LLM via Ollama")
        lines.append("- Report synthesis: Medical writing-focused language model")
        lines.append("")
        lines.append("**Quality Controls:**")
        lines.append("- Document ID verification prevents citation hallucination")
        lines.append("- Evidence strength assessment based on citation quality and quantity")
        lines.append("- Human-in-the-loop validation at each processing step")
        
        return lines
    
    def _get_counterfactual_analysis_section(self, analysis: CounterfactualAnalysis) -> List[str]:
        """Get counterfactual analysis section for the report."""
        lines = []
        
        lines.append("## Counterfactual Analysis")
        lines.append("")
        lines.append(f"**Original Confidence Level:** {analysis.confidence_level}")
        lines.append("")
        lines.append("### Main Claims Analyzed")
        lines.append("")
        for i, claim in enumerate(analysis.main_claims, 1):
            lines.append(f"{i}. {claim}")
        lines.append("")
        
        lines.append("### Research Questions for Contradictory Evidence")
        lines.append("")
        
        # Group questions by priority
        high_priority = [q for q in analysis.counterfactual_questions if q.priority == "HIGH"]
        medium_priority = [q for q in analysis.counterfactual_questions if q.priority == "MEDIUM"]
        low_priority = [q for q in analysis.counterfactual_questions if q.priority == "LOW"]
        
        if high_priority:
            lines.append("#### High Priority Questions")
            lines.append("")
            for i, question in enumerate(high_priority, 1):
                lines.append(f"**Question {i}:** {question.question}")
                lines.append("")
                lines.append(f"*Target Claim:* {question.target_claim}")
                lines.append("")
                lines.append(f"*Reasoning:* {question.reasoning}")
                lines.append("")
                lines.append(f"*Search Keywords:* {', '.join(question.search_keywords)}")
                lines.append("")
                lines.append("---")
                lines.append("")
        
        if medium_priority:
            lines.append("#### Medium Priority Questions")
            lines.append("")
            for i, question in enumerate(medium_priority, 1):
                lines.append(f"**Question {i}:** {question.question}")
                lines.append("")
                lines.append(f"*Target Claim:* {question.target_claim}")
                lines.append("")
                lines.append(f"*Search Keywords:* {', '.join(question.search_keywords)}")
                lines.append("")
        
        if low_priority:
            lines.append("#### Low Priority Questions")
            lines.append("")
            for i, question in enumerate(low_priority, 1):
                lines.append(f"**Question {i}:** {question.question}")
                lines.append("")
        
        lines.append("### Overall Assessment")
        lines.append("")
        lines.append(analysis.overall_assessment)
        lines.append("")
        
        return lines
    
    def save_report_to_file(self, report: Report, question: str, counterfactual_analysis: Optional[CounterfactualAnalysis] = None) -> bool:
        """Save the generated report as a markdown file with user interaction."""
        try:
            # Get filename from user
            filename = self.ui.get_report_filename(question)
            
            # Convert report to markdown format
            markdown_content = self.format_report_as_markdown(report, counterfactual_analysis)
            
            # Save file
            with open(filename, 'w', encoding='utf-8') as f:
                f.write(markdown_content)
            
            # Show confirmation
            file_size = os.path.getsize(filename)
            self.ui.show_file_saved(filename, file_size, markdown_content)
            
            return True
            
        except Exception as e:
            self.ui.show_error_message(f"Error saving report: {e}")
            return False
    
    def save_comprehensive_report_to_file(
        self, 
        report, 
        question: str, 
        counterfactual_analysis: Optional[CounterfactualAnalysis] = None,
        contradictory_evidence: Optional[Dict[str, Any]] = None,
        methodology_metadata: Optional[Any] = None
    ) -> bool:
        """Save comprehensive edited report or fallback to original report format."""
        try:
            # Get filename from user
            filename = self.ui.get_report_filename(question)
            
            # Handle different report types
            if isinstance(report, EditedReport):
                # Use the editor agent's template-based formatting for programmatic references and methodology
                from bmlibrarian.agents import EditorAgent
                editor_agent = EditorAgent()
                markdown_content = editor_agent.format_comprehensive_markdown_template(
                    report, 
                    methodology_metadata=methodology_metadata
                )
            else:
                # Fallback to original report format with enhanced content
                markdown_content = self.format_enhanced_report_as_markdown(
                    report, counterfactual_analysis, contradictory_evidence, methodology_metadata
                )
            
            # Save file
            with open(filename, 'w', encoding='utf-8') as f:
                f.write(markdown_content)
            
            # Show confirmation
            file_size = os.path.getsize(filename)
            self.ui.show_file_saved(filename, file_size, markdown_content)
            
            return filename
            
        except Exception as e:
            self.ui.show_error_message(f"Error saving comprehensive report: {e}")
            return False
    
    def format_enhanced_report_as_markdown(
        self, 
        report: Report, 
        counterfactual_analysis: Optional[CounterfactualAnalysis] = None,
        contradictory_evidence: Optional[Dict[str, Any]] = None,
        methodology_metadata: Optional[Any] = None
    ) -> str:
        """Format enhanced report with counterfactual analysis and contradictory evidence."""
        lines = []
        
        # Title and metadata
        lines.append(f"# Medical Literature Research Report (Enhanced)")
        lines.append("")
        lines.append(f"**Generated by BMLibrarian CLI (Enhanced Edition)**  ")
        lines.append(f"**Date:** {report.created_at.strftime('%Y-%m-%d %H:%M:%S UTC')}  ")
        lines.append(f"**Evidence Strength:** {report.evidence_strength}  ")
        if counterfactual_analysis:
            lines.append(f"**Original Confidence:** {counterfactual_analysis.confidence_level}  ")
            
        # Add revised confidence if contradictory evidence found
        if contradictory_evidence and contradictory_evidence.get('summary', {}).get('contradictory_citations_extracted', 0) > 0:
            revised_confidence = contradictory_evidence.get('summary', {}).get('revised_confidence', 'MEDIUM')
            lines.append(f"**Revised Confidence:** {revised_confidence} *(due to contradictory evidence)*  ")
        
        lines.append("")
        
        # Research question
        lines.append("## Research Question")
        lines.append("")
        lines.append(f"> {report.user_question}")
        lines.append("")
        
        # Evidence assessment with enhanced metrics
        lines.append("## Evidence Assessment")
        lines.append("")
        lines.append(f"- **Evidence Strength:** {report.evidence_strength}")
        lines.append(f"- **Citations Analyzed:** {report.citation_count}")
        lines.append(f"- **Unique References:** {report.unique_documents}")
        
        if counterfactual_analysis:
            lines.append(f"- **Main Claims Analyzed:** {len(counterfactual_analysis.main_claims)}")
            lines.append(f"- **Counterfactual Questions Generated:** {len(counterfactual_analysis.counterfactual_questions)}")
        
        if contradictory_evidence:
            lines.append(f"- **Contradictory Documents Found:** {contradictory_evidence.get('summary', {}).get('contradictory_documents_found', 0)}")
            lines.append(f"- **Contradictory Citations Extracted:** {contradictory_evidence.get('summary', {}).get('contradictory_citations_extracted', 0)}")
        
        lines.append("")
        
        # Main findings
        lines.append("## Findings")
        lines.append("")
        lines.append(report.synthesized_answer)
        lines.append("")
        
        # Contradictory evidence section (if available) - use new structured format if present
        if contradictory_evidence:
            # Check for new formatted_report structure
            if 'formatted_report' in contradictory_evidence:
                formatted_report = contradictory_evidence['formatted_report']
                lines.extend(self._format_structured_counterfactual_section(formatted_report))
            # Fallback to legacy format
            elif contradictory_evidence.get('contradictory_citations'):
                lines.append("## Contradictory Evidence")
                lines.append("")
                lines.append("The following evidence was found that may contradict or limit the main findings:")
                lines.append("")

                for i, item in enumerate(contradictory_evidence['contradictory_citations'][:5], 1):
                    citation = item.get('citation', {})
                    lines.append(f"### Contradictory Finding {i}")
                    lines.append("")
                    lines.append(f"**Target Claim:** {item.get('original_claim', 'N/A')}")
                    lines.append("")
                    if hasattr(citation, 'document_title'):
                        lines.append(f"**Source:** {citation.document_title}")
                        lines.append(f"**Evidence:** {getattr(citation, 'summary', 'N/A')}")
                        lines.append(f"**Relevance Score:** {item.get('document_score', 'N/A')}/5")
                    lines.append("")
        
        # Limitations section (enhanced)
        lines.append("## Limitations")
        lines.append("")
        lines.append("This analysis is based on available literature in the database and may have the following limitations:")
        lines.append("")
        lines.append("- Database coverage may not include all relevant studies")
        lines.append("- Search query optimization may affect result completeness")
        lines.append("- AI scoring systems may have inherent biases")
        lines.append("- Publication bias may affect available evidence")
        
        if counterfactual_analysis:
            lines.append("")
            lines.append("**Counterfactual Analysis Assessment:**")
            lines.append("")
            lines.append(counterfactual_analysis.overall_assessment)
        
        lines.append("")
        
        # References (programmatically formatted - NEVER let LLM generate these)
        if report.references:
            lines.append("## References")
            lines.append("")
            
            for ref in report.references:
                # Use proper Vancouver-style formatting with real DOI/PMID data
                if hasattr(ref, 'format_vancouver_style'):
                    formatted_ref = ref.format_vancouver_style()
                    lines.append(f"[{ref.number}] {formatted_ref}")
                else:
                    # Fallback to basic format if vancouver method not available
                    lines.append(f"[{ref.number}] {ref.title}")
                    lines.append(f"   Authors: {', '.join(ref.authors)}")
                    lines.append(f"   Date: {ref.publication_date}")
                lines.append("")
        
        # Methodology section - use programmatic metadata if available, otherwise fallback to enhanced
        if methodology_metadata:
            lines.append("## Methodology")
            lines.append("")
            # Use the same detailed methodology generation as our template system
            from ..agents.reporting_agent import ReportingAgent
            reporting_agent = ReportingAgent(show_model_info=False)
            methodology_section = reporting_agent.generate_detailed_methodology(methodology_metadata)
            lines.append(methodology_section)
            lines.append("")
        else:
            # Fallback to enhanced methodology 
            lines.extend(self._generate_enhanced_methodology_section())
        
        # Appendix with counterfactual questions (if available)
        if counterfactual_analysis and counterfactual_analysis.counterfactual_questions:
            lines.append("## Appendix: Counterfactual Research Questions")
            lines.append("")
            lines.append("The following research questions were generated to identify potential contradictory evidence:")
            lines.append("")
            
            # Group by priority
            high_priority = [q for q in counterfactual_analysis.counterfactual_questions if q.priority == "HIGH"]
            medium_priority = [q for q in counterfactual_analysis.counterfactual_questions if q.priority == "MEDIUM"]
            low_priority = [q for q in counterfactual_analysis.counterfactual_questions if q.priority == "LOW"]
            
            for priority_group, priority_name in [(high_priority, "High Priority"), (medium_priority, "Medium Priority"), (low_priority, "Low Priority")]:
                if priority_group:
                    lines.append(f"### {priority_name} Questions")
                    lines.append("")
                    for i, question in enumerate(priority_group, 1):
                        lines.append(f"{i}. **{question.question}**")
                        lines.append(f"   - Target: {question.target_claim}")
                        lines.append(f"   - Keywords: {', '.join(question.search_keywords)}")
                        lines.append("")
        
        return "\n".join(lines)
    
    def _generate_enhanced_methodology_section(self) -> List[str]:
        """Generate enhanced methodology section."""
        lines = []
        
        lines.append("## Methodology")
        lines.append("")
        lines.append("This enhanced report was generated using the BMLibrarian multi-agent system with the following advanced workflow:")
        lines.append("")
        
        lines.append("### Core Research Pipeline")
        lines.append("")
        lines.append("1. **Query Generation:** Natural language question converted to optimized PostgreSQL to_tsquery")
        lines.append("2. **Document Retrieval:** Full-text search with semantic ranking using pgvector extension")
        lines.append("3. **Relevance Scoring:** AI-powered document assessment using medical LLMs (1-5 scale)")
        lines.append("4. **Citation Extraction:** Automated extraction of relevant passages from high-scoring documents")
        lines.append("5. **Report Synthesis:** Medical publication-style report generation with evidence grading")
        lines.append("")
        
        lines.append("### Enhanced Analysis Pipeline")
        lines.append("")
        lines.append("6. **Counterfactual Analysis:** Identification of main claims and generation of research questions to find contradictory evidence")
        lines.append("7. **Contradictory Evidence Search:** Automated literature search for opposing findings using generated research questions")
        lines.append("8. **Comprehensive Synthesis:** Integration of supporting and contradictory evidence into balanced conclusions")
        lines.append("")
        
        lines.append("### Quality Controls")
        lines.append("")
        lines.append("- **Citation Verification:** Document ID validation prevents AI hallucination")
        lines.append("- **Evidence Grading:** Multi-tier assessment of evidence strength and reliability")
        lines.append("- **Bias Detection:** Counterfactual analysis identifies potential confirmation bias")
        lines.append("- **Human Oversight:** Interactive validation at critical processing steps")
        lines.append("- **Confidence Assessment:** Transparent reporting of evidence limitations")
        lines.append("")
        
        return lines

    def _format_structured_counterfactual_section(self, formatted_report: dict) -> List[str]:
        """
        Format the structured counterfactual report section for CLI output.

        Args:
            formatted_report: Dictionary with items, summary_statement, and statistics

        Returns:
            List of formatted lines for the counterfactual section
        """
        lines = []
        lines.append("")
        lines.append("## Counterfactual Evidence Analysis")
        lines.append("")

        items = formatted_report.get('items', [])
        summary_statement = formatted_report.get('summary_statement', '')

        if not items:
            lines.append("No claims were analyzed for counterfactual evidence.")
            return lines

        # Count claims with and without evidence
        claims_with_evidence = [item for item in items if item.get('evidence_found', False)]
        claims_without_evidence = [item for item in items if not item.get('evidence_found', False)]

        lines.append(f"This analysis examined **{len(items)} claims** from the original report:")
        lines.append(f"- {len(claims_with_evidence)} claims have contradictory evidence")
        lines.append(f"- {len(claims_without_evidence)} claims have no contradictory evidence found")
        lines.append("")

        # Format each claim
        for i, item in enumerate(items, 1):
            claim = item.get('claim', 'Unknown claim')
            counterfactual_statement = item.get('counterfactual_statement', '')
            counterfactual_question = item.get('counterfactual_question', '')
            evidence_list = item.get('counterfactual_evidence', [])
            evidence_found = item.get('evidence_found', False)
            critical_assessment = item.get('critical_assessment', '')

            lines.append(f"### Claim {i}")
            lines.append("")
            lines.append(f"**Original Claim:** {claim}")
            lines.append("")
            lines.append(f"**Counterfactual Statement:** {counterfactual_statement}")
            lines.append("")
            if counterfactual_question:
                lines.append(f"**Research Question:** {counterfactual_question}")
                lines.append("")

            if evidence_found and evidence_list:
                lines.append(f"**Contradictory Evidence:** {len(evidence_list)} citation(s) found")
                lines.append("")

                for j, evidence in enumerate(evidence_list, 1):
                    title = evidence.get('title', 'Unknown title')
                    citation_content = evidence.get('content', 'No content available')
                    passage = evidence.get('passage', '')
                    relevance_score = evidence.get('relevance_score', 0)
                    document_score = evidence.get('document_score', 0)

                    lines.append(f"{j}. **{title}**")
                    lines.append(f"   - Relevance: Citation {relevance_score:.2f}/1.0, Document {document_score}/5")
                    lines.append(f"   - Summary: {citation_content}")
                    if passage:
                        display_passage = passage[:300] + "..." if len(passage) > 300 else passage
                        lines.append(f"   - Key Passage: \"{display_passage}\"")
                    lines.append("")

                lines.append(f"**Critical Assessment:** {critical_assessment}")
            else:
                lines.append(f"**Contradictory Evidence:** None found")
                lines.append("")
                lines.append(f"**Assessment:** {critical_assessment}")

            lines.append("")

        # Overall summary
        if summary_statement:
            lines.append("### Overall Summary")
            lines.append("")
            lines.append(summary_statement)
            lines.append("")

        return lines

    def format_citation_summary(self, citations: List[Citation]) -> str:
        """Format a summary of citations for display."""
        if not citations:
            return "No citations available."
        
        lines = []
        lines.append(f"Citation Summary ({len(citations)} citations):")
        lines.append("=" * 50)
        
        # Group citations by document
        doc_citations = {}
        for citation in citations:
            doc_id = citation.document_id
            if doc_id not in doc_citations:
                doc_citations[doc_id] = []
            doc_citations[doc_id].append(citation)
        
        # Format by document
        for doc_id, doc_citations_list in doc_citations.items():
            first_citation = doc_citations_list[0]
            lines.append(f"\n📄 {first_citation.document_title}")
            lines.append(f"   Authors: {', '.join(first_citation.authors[:3])}{'...' if len(first_citation.authors) > 3 else ''}")
            lines.append(f"   Date: {first_citation.publication_date}")
            lines.append(f"   Citations from this document: {len(doc_citations_list)}")
            
            # Show highest relevance citation
            best_citation = max(doc_citations_list, key=lambda c: c.relevance_score)
            lines.append(f"   Best passage (relevance: {best_citation.relevance_score:.3f}):")
            lines.append(f"   \"{best_citation.passage[:100]}...\"")
        
        return "\n".join(lines)
    
    def export_citations_to_csv(self, citations: List[Citation], filename: str = None) -> bool:
        """Export citations to CSV format."""
        try:
            import csv
            
            if filename is None:
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                filename = f"bmlibrarian_citations_{timestamp}.csv"
            
            with open(filename, 'w', newline='', encoding='utf-8') as csvfile:
                fieldnames = [
                    'document_id', 'document_title', 'authors', 'publication_date',
                    'passage', 'summary', 'relevance_score'
                ]
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                
                writer.writeheader()
                for citation in citations:
                    writer.writerow({
                        'document_id': citation.document_id,
                        'document_title': citation.document_title,
                        'authors': '; '.join(citation.authors),
                        'publication_date': citation.publication_date,
                        'passage': citation.passage,
                        'summary': citation.summary,
                        'relevance_score': citation.relevance_score
                    })
            
            self.ui.show_success_message(f"Citations exported to: {filename}")
            return True
            
        except Exception as e:
            self.ui.show_error_message(f"Error exporting citations: {e}")
            return False
    
    def export_citations_to_json(self, citations: List[Citation], filename: str = None) -> bool:
        """Export citations to JSON format."""
        try:
            import json
            
            if filename is None:
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                filename = f"bmlibrarian_citations_{timestamp}.json"
            
            citations_data = []
            for citation in citations:
                citations_data.append({
                    'document_id': citation.document_id,
                    'document_title': citation.document_title,
                    'authors': citation.authors,
                    'publication_date': citation.publication_date,
                    'passage': citation.passage,
                    'summary': citation.summary,
                    'relevance_score': citation.relevance_score
                })
            
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(citations_data, f, indent=2, ensure_ascii=False)
            
            self.ui.show_success_message(f"Citations exported to: {filename}")
            return True
            
        except Exception as e:
            self.ui.show_error_message(f"Error exporting citations: {e}")
            return False
    
    def create_citation_bibliography(self, citations: List[Citation], 
                                   style: str = 'vancouver') -> str:
        """Create a bibliography from citations in specified style."""
        if not citations:
            return ""
        
        # Group citations by document to avoid duplicates
        unique_docs = {}
        for citation in citations:
            doc_id = citation.document_id
            if doc_id not in unique_docs:
                unique_docs[doc_id] = citation
        
        lines = []
        if style.lower() == 'vancouver':
            lines.append("## References (Vancouver Style)")
            lines.append("")
            
            for i, (doc_id, citation) in enumerate(unique_docs.items(), 1):
                ref_line = f"{i}. {self._format_vancouver_citation(citation)}"
                lines.append(ref_line)
        
        elif style.lower() == 'apa':
            lines.append("## References (APA Style)")
            lines.append("")
            
            # Sort by first author's last name for APA
            sorted_citations = sorted(unique_docs.values(), 
                                    key=lambda c: c.authors[0].split()[-1] if c.authors else '')
            
            for citation in sorted_citations:
                ref_line = self._format_apa_citation(citation)
                lines.append(ref_line)
                lines.append("")  # APA style uses line breaks between references
        
        return "\n".join(lines)
    
    def _format_vancouver_citation(self, citation: Citation) -> str:
        """Format a single citation in Vancouver style."""
        authors = citation.authors[:6]  # Vancouver typically shows up to 6 authors
        
        if len(authors) > 6:
            author_str = ", ".join(authors[:6]) + ", et al."
        else:
            author_str = ", ".join(authors)
        
        # Basic Vancouver format: Authors. Title. Journal. Year;Volume(Issue):Pages.
        parts = [author_str]
        
        if citation.document_title:
            parts.append(citation.document_title)
        
        # Add publication date if available
        if citation.publication_date:
            year = self._extract_year_from_date(citation.publication_date)
            if year:
                parts.append(f"{year}")
        
        return ". ".join(filter(None, parts)) + "."
    
    def _format_apa_citation(self, citation: Citation) -> str:
        """Format a single citation in APA style."""
        authors = citation.authors
        
        # Format authors (Last, F. M.)
        if authors:
            if len(authors) == 1:
                author_str = self._format_apa_author(authors[0])
            elif len(authors) <= 20:
                formatted_authors = [self._format_apa_author(author) for author in authors[:-1]]
                author_str = ", ".join(formatted_authors) + f", & {self._format_apa_author(authors[-1])}"
            else:
                # For more than 20 authors, show first 19, then "...", then last
                formatted_authors = [self._format_apa_author(author) for author in authors[:19]]
                author_str = ", ".join(formatted_authors) + f", ... {self._format_apa_author(authors[-1])}"
        else:
            author_str = "Unknown"
        
        # Add year
        year = self._extract_year_from_date(citation.publication_date) or "n.d."
        
        # Add title
        title = citation.document_title if citation.document_title else "Untitled"
        
        return f"{author_str} ({year}). {title}."
    
    def _format_apa_author(self, author_name: str) -> str:
        """Format a single author name for APA style."""
        # Split name into parts
        parts = author_name.strip().split()
        if len(parts) == 1:
            return parts[0]
        elif len(parts) == 2:
            # First Last -> Last, F.
            return f"{parts[-1]}, {parts[0][0]}."
        else:
            # First Middle Last -> Last, F. M.
            initials = " ".join([name[0] + "." for name in parts[:-1]])
            return f"{parts[-1]}, {initials}"
    
    def _extract_year_from_date(self, date_str: str) -> Optional[str]:
        """Extract year from date string."""
        if not date_str:
            return None
        
        import re
        year_match = re.search(r'\b(19|20)\d{2}\b', str(date_str))
        if year_match:
            return year_match.group()
        return None
    
    def generate_report_template(self, question: str) -> str:
        """Generate a basic report template for manual completion."""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        template = f"""# Medical Literature Research Report

**Generated by BMLibrarian CLI**  
**Date:** {timestamp}  
**Evidence Strength:** [To be determined]  

## Research Question

> {question}

## Evidence Assessment

- **Evidence Strength:** [To be determined]
- **Citations Analyzed:** [To be determined]
- **Unique References:** [To be determined]

## Findings

[Research findings will be generated here based on extracted citations]

## References

[References will be added during report generation]

## Methodology

This report was generated using the BMLibrarian multi-agent system with the following steps:

1. **Query Generation:** Natural language question converted to database query
2. **Document Retrieval:** PostgreSQL full-text search with pgvector extension
3. **Relevance Scoring:** AI-powered document scoring (1-5 scale)
4. **Citation Extraction:** Relevant passage extraction from high-scoring documents
5. **Report Synthesis:** Medical publication-style report generation

**Quality Controls:**
- Document ID verification prevents citation hallucination
- Evidence strength assessment based on citation quality and quantity
- Human-in-the-loop validation at each processing step
"""
        return template
    
    def validate_filename(self, filename: str) -> bool:
        """Validate filename for safety and compatibility."""
        try:
            # Check for valid characters
            invalid_chars = '<>:"|?*'
            if any(char in filename for char in invalid_chars):
                return False
            
            # Check length
            if len(filename) > 255:
                return False
            
            # Check for reserved names (Windows)
            reserved_names = ['CON', 'PRN', 'AUX', 'NUL', 'COM1', 'COM2', 'COM3', 'COM4', 
                            'COM5', 'COM6', 'COM7', 'COM8', 'COM9', 'LPT1', 'LPT2', 'LPT3', 
                            'LPT4', 'LPT5', 'LPT6', 'LPT7', 'LPT8', 'LPT9']
            
            name_without_ext = Path(filename).stem.upper()
            if name_without_ext in reserved_names:
                return False
            
            return True
            
        except:
            return False
    
    def create_backup_filename(self, original_filename: str) -> str:
        """Create a backup filename if the original exists."""
        path = Path(original_filename)
        counter = 1
        
        while path.exists():
            stem = path.stem
            suffix = path.suffix
            
            # Remove any existing counter
            if stem.endswith(f"_{counter-1}") and counter > 1:
                stem = stem[:-len(f"_{counter-1}")]
            
            new_filename = f"{stem}_{counter}{suffix}"
            path = Path(new_filename)
            counter += 1
        
        return str(path)