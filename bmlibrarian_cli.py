#!/usr/bin/env python3
"""
BMLibrarian CLI - Interactive Medical Literature Research Tool

A comprehensive command-line interface for conducting evidence-based medical literature research
using the full BMLibrarian multi-agent workflow with human-in-the-loop interaction.

Features:
- Interactive query generation and editing
- Database search with real-time results
- Document relevance scoring with user review
- Citation extraction from high-scoring documents
- Medical publication-style report generation
- Markdown report export with proper formatting

Usage:
    python bmlibrarian_cli.py

Requirements:
- PostgreSQL database with biomedical literature
- Ollama service running locally
- BMLibrarian agents properly configured
"""

import os
import sys
import json
import time
import tempfile
import argparse
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime

# Add src to path for imports
sys.path.insert(0, str(Path(__file__).parent / "src"))

from bmlibrarian.agents import (
    QueryAgent, DocumentScoringAgent, CitationFinderAgent, 
    ReportingAgent, AgentOrchestrator, Citation, Report
)


class MedicalResearchCLI:
    """Interactive CLI for medical literature research using BMLibrarian agents."""
    
    def __init__(self):
        """Initialize CLI with agent orchestrator and setup."""
        print("üè• BMLibrarian Medical Research CLI")
        print("=" * 60)
        
        self.orchestrator = None
        self.query_agent = None
        self.scoring_agent = None
        self.citation_agent = None
        self.reporting_agent = None
        
        # Session state
        self.current_question = None
        self.current_query = None
        self.search_results = []
        self.scored_documents = []
        self.extracted_citations = []
        self.final_report = None
        
        # Configuration
        self.max_documents_display = 10
        self.default_score_threshold = 2.5
        self.default_min_relevance = 0.7
        self.max_search_results = 100  # Default max search results
        self.timeout_minutes = 5  # Default timeout in minutes
    
    def test_database_connection(self) -> bool:
        """Test database connection."""
        try:
            from bmlibrarian.database import get_db_manager
            
            db_manager = get_db_manager()
            with db_manager.get_connection() as conn:
                with conn.cursor() as cur:
                    cur.execute("SELECT 1")
                    result = cur.fetchone()
                    return result is not None
        except Exception as e:
            print(f"   Database connection error: {e}")
            return False
        
    def setup_agents(self) -> bool:
        """Initialize and test all agents."""
        try:
            print("\nüîß Setting up BMLibrarian agents...")
            
            # Initialize orchestrator
            self.orchestrator = AgentOrchestrator(max_workers=4, polling_interval=0.5)
            
            # Initialize agents
            self.query_agent = QueryAgent(orchestrator=self.orchestrator)
            self.scoring_agent = DocumentScoringAgent(orchestrator=self.orchestrator)
            self.citation_agent = CitationFinderAgent(orchestrator=self.orchestrator)
            self.reporting_agent = ReportingAgent(orchestrator=self.orchestrator)
            
            # Register agents
            self.orchestrator.register_agent("query_agent", self.query_agent)
            self.orchestrator.register_agent("document_scoring_agent", self.scoring_agent)
            self.orchestrator.register_agent("citation_finder_agent", self.citation_agent)
            self.orchestrator.register_agent("reporting_agent", self.reporting_agent)
            
            print("   ‚úÖ Agents initialized")
            
            # Test connections
            print("\nüîç Testing service connections...")
            
            # Test database connection
            db_connected = self.test_database_connection()
            print(f"   Database: {'‚úÖ Connected' if db_connected else '‚ùå Failed'}")
            
            # Test Ollama connections
            scoring_connected = self.scoring_agent.test_connection()
            print(f"   Scoring Agent (Ollama): {'‚úÖ Connected' if scoring_connected else '‚ùå Failed'}")
            
            citation_connected = self.citation_agent.test_connection()
            print(f"   Citation Agent (Ollama): {'‚úÖ Connected' if citation_connected else '‚ùå Failed'}")
            
            reporting_connected = self.reporting_agent.test_connection()
            print(f"   Reporting Agent (Ollama): {'‚úÖ Connected' if reporting_connected else '‚ùå Failed'}")
            
            # Check if all critical services are available
            if not (scoring_connected and citation_connected and reporting_connected):
                print("\n‚ö†Ô∏è  Some AI services are unavailable. Please ensure:")
                print("   - Ollama is running: ollama serve")
                print("   - Required models are installed:")
                print("     ollama pull gpt-oss:20b")
                print("     ollama pull medgemma4B_it_q8:latest")
                return False
            
            print("\n‚úÖ All services connected and ready!")
            return True
            
        except Exception as e:
            print(f"\n‚ùå Failed to setup agents: {e}")
            return False
    
    def get_user_question(self) -> str:
        """Get research question from user with validation."""
        print("\n" + "=" * 60)
        print("Step 1: Research Question")
        print("=" * 60)
        
        while True:
            print("\nPlease enter your medical research question:")
            print("Examples:")
            print("‚Ä¢ What are the cardiovascular benefits of exercise?")
            print("‚Ä¢ How effective is metformin for diabetes management?")
            print("‚Ä¢ What are the side effects of statins in elderly patients?")
            
            question = input("\nüî¨ Your question: ").strip()
            
            if not question:
                print("‚ùå Please enter a valid question.")
                continue
            
            if len(question) < 10:
                print("‚ùå Please provide a more detailed question (at least 10 characters).")
                continue
            
            # Confirm question
            print(f"\nüìã Your research question: \"{question}\"")
            confirm = input("Is this correct? (y/n): ").strip().lower()
            
            if confirm in ['y', 'yes']:
                self.current_question = question
                return question
            
            print("Let's try again...")
    
    def search_documents_with_review(self, question: str) -> List[Dict[str, Any]]:
        """Use QueryAgent to search documents and allow user review."""
        print("\n" + "=" * 60)
        print("Step 2: Document Search")
        print("=" * 60)
        
        try:
            print(f"\nüîç Searching for: \"{question}\"")
            print("‚è≥ Processing...")
            
            # Use QueryAgent's find_abstracts method directly
            documents = []
            for doc in self.query_agent.find_abstracts(
                question=question,
                max_rows=self.max_search_results,
                use_ranking=True
            ):
                documents.append(doc)
            
            if not documents:
                print("‚ùå No documents found.")
                return []
            
            print(f"\n‚úÖ Found {len(documents)} documents")
            return documents
            
        except Exception as e:
            print(f"‚ùå Error in document search: {e}")
            return []
    
    def display_documents(self, documents: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Display documents to user and get confirmation to proceed."""
        if not documents:
            return documents
        
        # Display sample results
        print(f"\nüìÑ Showing first {min(self.max_documents_display, len(documents))} documents:")
        print("=" * 80)
        
        for i, doc in enumerate(documents[:self.max_documents_display], 1):
            print(f"\n{i}. Title: {doc.get('title', 'No title')}")
            print(f"   ID: {doc.get('id', 'Unknown')}")
            authors = doc.get('authors', [])
            if isinstance(authors, list):
                authors_str = ', '.join(authors)[:100]
            else:
                authors_str = str(authors)[:100]
            print(f"   Authors: {authors_str}")
            print(f"   Date: {doc.get('publication_date', 'Unknown')}")
            
            # Show abstract preview
            abstract = doc.get('abstract', '')
            if abstract:
                preview = abstract[:200] + "..." if len(abstract) > 200 else abstract
                print(f"   Abstract: {preview}")
            else:
                print("   Abstract: Not available")
            
            if doc.get('pmid'):
                print(f"   PMID: {doc['pmid']}")
            
            print("-" * 80)
        
        if len(documents) > self.max_documents_display:
            print(f"\n... and {len(documents) - self.max_documents_display} more documents")
        
        # Ask user if they want to proceed
        while True:
            print(f"\nFound {len(documents)} total documents.")
            print("Options:")
            print("1. Proceed with these results")
            print("2. Search again with different terms")
            print("3. Show more document details")
            
            choice = input("Choose option (1-3): ").strip()
            
            if choice == '1':
                self.search_results = documents
                return documents
            
            elif choice == '2':
                # Get new question and search again
                new_question = self.get_user_question()
                return self.search_documents_with_review(new_question)
            
            elif choice == '3':
                # Show detailed view of documents
                self.show_detailed_documents(documents)
                continue
            
            else:
                print("‚ùå Invalid option. Please choose 1-3.")
    
    def show_detailed_documents(self, documents: List[Dict[str, Any]]):
        """Show detailed view of documents."""
        start_idx = 0
        while start_idx < len(documents):
            end_idx = min(start_idx + 5, len(documents))
            print(f"\nüìã Documents {start_idx + 1}-{end_idx} of {len(documents)}:")
            print("=" * 80)
            
            for i in range(start_idx, end_idx):
                doc = documents[i]
                print(f"\n{i + 1}. {doc.get('title', 'No title')}")
                print(f"    ID: {doc.get('id')}")
                authors = doc.get('authors', [])
                if isinstance(authors, list):
                    authors_str = ', '.join(authors)
                else:
                    authors_str = str(authors)
                print(f"    Authors: {authors_str}")
                print(f"    Date: {doc.get('publication_date', 'Unknown')}")
                print(f"    Abstract: {doc.get('abstract', 'Not available')}")
                print("-" * 80)
            
            if end_idx < len(documents):
                cont = input(f"\nShow next 5 documents? (y/n): ").strip().lower()
                if cont not in ['y', 'yes']:
                    break
                start_idx = end_idx
            else:
                break
    
    def score_documents_with_review(self, question: str, documents: List[Dict[str, Any]]) -> List[Tuple[Dict[str, Any], Dict[str, Any]]]:
        """Score documents and allow user to review scores."""
        print("\n" + "=" * 60)
        print("Step 4: Document Relevance Scoring")
        print("=" * 60)
        
        try:
            print(f"\nüéØ Scoring {len(documents)} documents for relevance to:")
            print(f'   "{question}"')
            print("\n‚è≥ This may take a few minutes...")
            
            scored_docs = []
            
            # Score documents with progress indication
            for i, doc in enumerate(documents, 1):
                print(f"   Scoring document {i}/{len(documents)}: {doc.get('title', 'Untitled')[:50]}...")
                
                score_result = self.scoring_agent.evaluate_document(question, doc)
                
                if score_result:
                    scored_docs.append((doc, score_result))
                else:
                    print(f"   ‚ö†Ô∏è  Failed to score document {i}")
            
            if not scored_docs:
                print("‚ùå No documents could be scored. Check Ollama connection.")
                return []
            
            # Sort by score (descending)
            scored_docs.sort(key=lambda x: x[1].get('score', 0), reverse=True)
            
            print(f"\n‚úÖ Successfully scored {len(scored_docs)} documents")
            
            # Display scoring results
            print("\nüìä Document Scores (1-5 scale, 5=most relevant):")
            print("=" * 80)
            
            for i, (doc, score_result) in enumerate(scored_docs[:10], 1):
                score = score_result.get('score', 0)
                reasoning = score_result.get('reasoning', 'No reasoning provided')
                
                # Color coding for scores
                if score >= 4:
                    score_display = f"üü¢ {score}/5"
                elif score >= 3:
                    score_display = f"üü° {score}/5"
                elif score >= 2:
                    score_display = f"üü† {score}/5"
                else:
                    score_display = f"üî¥ {score}/5"
                
                print(f"\n{i}. {score_display} - {doc.get('title', 'No title')[:60]}")
                print(f"   ID: {doc.get('id')}")
                print(f"   Reasoning: {reasoning}")
                print("-" * 80)
            
            if len(scored_docs) > 10:
                print(f"\n... and {len(scored_docs) - 10} more scored documents")
            
            # Show score distribution
            score_counts = {}
            for _, score_result in scored_docs:
                score = int(score_result.get('score', 0))
                score_counts[score] = score_counts.get(score, 0) + 1
            
            print(f"\nüìà Score Distribution:")
            for score in range(5, 0, -1):
                count = score_counts.get(score, 0)
                bar = "‚ñà" * min(count, 20)
                print(f"   {score}/5: {count:3d} docs {bar}")
            
            # Ask about score threshold
            while True:
                print(f"\n‚öôÔ∏è  Configuration:")
                high_scoring = len([doc for doc, score in scored_docs if score.get('score', 0) > self.default_score_threshold])
                print(f"   Current threshold: {self.default_score_threshold}")
                print(f"   Documents above threshold: {high_scoring}")
                
                print("\nOptions:")
                print("1. Proceed with current threshold")
                print("2. Adjust score threshold")
                print("3. Review individual scores")
                print("4. Re-score with different parameters")
                
                choice = input("Choose option (1-4): ").strip()
                
                if choice == '1':
                    self.scored_documents = scored_docs
                    return scored_docs
                
                elif choice == '2':
                    while True:
                        try:
                            new_threshold = float(input(f"Enter new threshold (current: {self.default_score_threshold}): "))
                            if 0 <= new_threshold <= 5:
                                self.default_score_threshold = new_threshold
                                qualifying = len([doc for doc, score in scored_docs if score.get('score', 0) > new_threshold])
                                print(f"‚úÖ New threshold: {new_threshold}")
                                print(f"   Documents that will qualify: {qualifying}")
                                break
                            else:
                                print("‚ùå Threshold must be between 0 and 5")
                        except ValueError:
                            print("‚ùå Please enter a valid number")
                    continue
                
                elif choice == '3':
                    # Show detailed score review
                    self.show_detailed_scores(scored_docs)
                    continue
                
                elif choice == '4':
                    print("Re-scoring documents...")
                    # Could implement different scoring parameters here
                    continue
                
                else:
                    print("‚ùå Invalid option. Please choose 1-4.")
        
        except Exception as e:
            print(f"‚ùå Error in document scoring: {e}")
            return []
    
    def show_detailed_scores(self, scored_docs: List[Tuple[Dict[str, Any], Dict[str, Any]]]):
        """Show detailed view of document scores."""
        print("\nüìã Detailed Score Review:")
        print("=" * 80)
        
        for i, (doc, score_result) in enumerate(scored_docs, 1):
            print(f"\n{i}. Score: {score_result.get('score', 0)}/5")
            print(f"   Title: {doc.get('title', 'No title')}")
            print(f"   Authors: {', '.join(doc.get('authors', []))}")
            print(f"   Date: {doc.get('publication_date', 'Unknown')}")
            print(f"   Reasoning: {score_result.get('reasoning', 'No reasoning')}")
            print(f"   Abstract preview: {doc.get('abstract', '')[:150]}...")
            print("-" * 80)
            
            if i % 5 == 0 and i < len(scored_docs):
                cont = input(f"\nContinue viewing? ({i}/{len(scored_docs)}) (y/n): ").strip().lower()
                if cont not in ['y', 'yes']:
                    break
    
    def extract_citations_with_review(self, question: str, scored_docs: List[Tuple[Dict[str, Any], Dict[str, Any]]]) -> List[Citation]:
        """Extract citations and allow user review."""
        print("\n" + "=" * 60)
        print("Step 5: Citation Extraction")
        print("=" * 60)
        
        try:
            # Filter documents above threshold
            qualifying_docs = [
                (doc, score) for doc, score in scored_docs 
                if score.get('score', 0) > self.default_score_threshold
            ]
            
            print(f"\nüìÑ Processing {len(qualifying_docs)} documents above threshold {self.default_score_threshold}")
            print(f'   Extracting citations for: "{question}"')
            print("\n‚è≥ This may take several minutes...")
            
            # Extract citations with progress indication
            def progress_callback(current, total):
                percentage = (current / total) * 100
                print(f"   Progress: {current}/{total} documents ({percentage:.1f}%)")
            
            citations = self.citation_agent.process_scored_documents_for_citations(
                user_question=question,
                scored_documents=qualifying_docs,
                score_threshold=self.default_score_threshold,
                min_relevance=self.default_min_relevance,
                progress_callback=progress_callback
            )
            
            if not citations:
                print("‚ùå No citations extracted.")
                print("\nPossible reasons:")
                print("‚Ä¢ Score threshold too high")
                print("‚Ä¢ Minimum relevance threshold too high")
                print("‚Ä¢ Documents don't contain relevant passages")
                print("‚Ä¢ Ollama connection issues")
                return []
            
            print(f"\n‚úÖ Extracted {len(citations)} citations")
            
            # Display citations
            print("\nüìù Extracted Citations:")
            print("=" * 80)
            
            for i, citation in enumerate(citations, 1):
                print(f"\n{i}. Relevance: {citation.relevance_score:.3f}")
                print(f"   Document: {citation.document_title[:70]}")
                print(f"   Authors: {', '.join(citation.authors[:3])}{'...' if len(citation.authors) > 3 else ''}")
                print(f"   Passage: \"{citation.passage}\"")
                print(f"   Summary: {citation.summary}")
                print(f"   Document ID: {citation.document_id}")
                print("-" * 80)
            
            # Show citation statistics
            stats = self.citation_agent.get_citation_stats(citations)
            print(f"\nüìä Citation Statistics:")
            print(f"   Total citations: {stats['total_citations']}")
            print(f"   Unique documents: {stats['unique_documents']}")
            print(f"   Average relevance: {stats['average_relevance']:.3f}")
            print(f"   Relevance range: {stats['min_relevance']:.3f} - {stats['max_relevance']:.3f}")
            
            while True:
                print(f"\nCitation options:")
                print("1. Proceed with these citations")
                print("2. Adjust relevance threshold")
                print("3. Review individual citations")
                print("4. Go back to adjust document scores")
                
                choice = input("Choose option (1-4): ").strip()
                
                if choice == '1':
                    self.extracted_citations = citations
                    return citations
                
                elif choice == '2':
                    while True:
                        try:
                            new_relevance = float(input(f"Enter new minimum relevance (current: {self.default_min_relevance}): "))
                            if 0 <= new_relevance <= 1:
                                self.default_min_relevance = new_relevance
                                print(f"‚úÖ New minimum relevance: {new_relevance}")
                                
                                # Re-extract with new threshold
                                return self.extract_citations_with_review(question, scored_docs)
                            else:
                                print("‚ùå Relevance must be between 0 and 1")
                        except ValueError:
                            print("‚ùå Please enter a valid number")
                
                elif choice == '3':
                    # Show detailed citation review
                    for i, citation in enumerate(citations, 1):
                        print(f"\n{'='*60}")
                        print(f"Citation {i} of {len(citations)}")
                        print(f"{'='*60}")
                        print(f"Document: {citation.document_title}")
                        print(f"Authors: {', '.join(citation.authors)}")
                        print(f"Date: {citation.publication_date}")
                        print(f"Relevance Score: {citation.relevance_score:.3f}")
                        print(f"Document ID: {citation.document_id}")
                        print(f"\nPassage:")
                        print(f'"{citation.passage}"')
                        print(f"\nSummary:")
                        print(f"{citation.summary}")
                        
                        if i < len(citations):
                            cont = input(f"\nContinue to next citation? (y/n): ").strip().lower()
                            if cont not in ['y', 'yes']:
                                break
                    continue
                
                elif choice == '4':
                    # Return to document scoring
                    return self.score_documents_with_review(question, [(doc, score) for doc, score in scored_docs])
                
                else:
                    print("‚ùå Invalid option. Please choose 1-4.")
        
        except Exception as e:
            print(f"‚ùå Error in citation extraction: {e}")
            return []
    
    def generate_final_report(self, question: str, citations: List[Citation]) -> Optional[Report]:
        """Generate final medical publication-style report."""
        print("\n" + "=" * 60)
        print("Step 6: Report Generation")
        print("=" * 60)
        
        try:
            print(f"\nüìÑ Generating medical publication-style report...")
            print(f'   Research question: "{question}"')
            print(f"   Based on {len(citations)} citations")
            
            # Check if we have too many citations for reliable processing
            if len(citations) > 20:
                print(f"\n‚ö†Ô∏è  You have {len(citations)} citations. Large citation sets may:")
                print("‚Ä¢ Take a long time to process (5-10 minutes)")
                print("‚Ä¢ Cause timeouts with the AI model")
                print("‚Ä¢ Produce less focused reports")
                
                while True:
                    print(f"\nOptions:")
                    print("1. Proceed with all citations (may be slow)")
                    print("2. Use only the top citations by relevance")
                    print("3. Go back and increase citation thresholds")
                    
                    choice = input("Choose option (1-3): ").strip()
                    
                    if choice == '1':
                        break
                    elif choice == '2':
                        # Sort by relevance and take top 15
                        sorted_citations = sorted(citations, key=lambda c: c.relevance_score, reverse=True)
                        citations = sorted_citations[:15]
                        print(f"‚úÖ Using top {len(citations)} citations by relevance score")
                        break
                    elif choice == '3':
                        print("Please go back to citation extraction step and adjust thresholds.")
                        return None
                    else:
                        print("‚ùå Invalid option. Please choose 1-3.")
            
            print(f"\n‚è≥ Synthesizing evidence from {len(citations)} citations...")
            print("   Using iterative processing to avoid context limits...")
            print("   Processing citations one by one - this may take a few minutes...")
            
            # Generate report using iterative approach
            report = self.reporting_agent.synthesize_report(
                user_question=question,
                citations=citations,
                min_citations=2
            )
            
            if not report:
                print("‚ùå Failed to generate report after multiple attempts.")
                print("Suggestions:")
                print("‚Ä¢ Try with fewer citations (go back to citation step)")
                print("‚Ä¢ Check Ollama model performance")
                print("‚Ä¢ Ensure sufficient system resources")
                return None
            
            print(f"\n‚úÖ Report generated successfully!")
            print(f"   Evidence strength: {report.evidence_strength}")
            print(f"   Citations analyzed: {report.citation_count}")
            print(f"   Unique references: {report.unique_documents}")
            
            # Format and display report
            formatted_report = self.reporting_agent.format_report_output(report)
            
            print("\n" + "=" * 80)
            print("GENERATED RESEARCH REPORT")
            print("=" * 80)
            print(formatted_report)
            print("=" * 80)
            
            self.final_report = report
            return report
            
        except Exception as e:
            print(f"‚ùå Error generating report: {e}")
            print("\nSuggestions:")
            print("‚Ä¢ Reduce the number of citations")
            print("‚Ä¢ Check Ollama service is running properly")
            print("‚Ä¢ Ensure sufficient memory and processing power")
            return None
    
    def save_report_as_markdown(self, report: Report) -> bool:
        """Save the generated report as a markdown file."""
        print("\n" + "=" * 60)
        print("Step 7: Save Report")
        print("=" * 60)
        
        try:
            # Generate filename
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            question_slug = "".join(c if c.isalnum() or c in [' ', '-'] else '' for c in self.current_question)
            question_slug = "_".join(question_slug.split()[:5])  # First 5 words
            
            default_filename = f"bmlibrarian_report_{question_slug}_{timestamp}.md"
            
            print(f"\nüíæ Save report as markdown file:")
            filename = input(f"Filename (default: {default_filename}): ").strip()
            
            if not filename:
                filename = default_filename
            
            if not filename.endswith('.md'):
                filename += '.md'
            
            # Convert report to markdown format
            markdown_content = self.format_report_as_markdown(report)
            
            # Save file
            with open(filename, 'w', encoding='utf-8') as f:
                f.write(markdown_content)
            
            print(f"‚úÖ Report saved as: {filename}")
            print(f"   File size: {os.path.getsize(filename)} bytes")
            
            # Show file preview
            print(f"\nüìÑ File preview (first 300 characters):")
            print("-" * 50)
            print(markdown_content[:300] + ("..." if len(markdown_content) > 300 else ""))
            print("-" * 50)
            
            return True
            
        except Exception as e:
            print(f"‚ùå Error saving report: {e}")
            return False
    
    def format_report_as_markdown(self, report: Report) -> str:
        """Format report as markdown with proper structure."""
        lines = []
        
        # Title and metadata
        lines.append(f"# Medical Literature Research Report")
        lines.append("")
        lines.append(f"**Generated by BMLibrarian CLI**  ")
        lines.append(f"**Date:** {report.created_at.strftime('%Y-%m-%d %H:%M:%S UTC')}  ")
        lines.append(f"**Evidence Strength:** {report.evidence_strength}  ")
        lines.append("")
        
        # Research question
        lines.append("## Research Question")
        lines.append("")
        lines.append(f"> {report.user_question}")
        lines.append("")
        
        # Evidence assessment
        lines.append("## Evidence Assessment")
        lines.append("")
        lines.append(f"- **Evidence Strength:** {report.evidence_strength}")
        lines.append(f"- **Citations Analyzed:** {report.citation_count}")
        lines.append(f"- **Unique References:** {report.unique_documents}")
        lines.append("")
        
        # Synthesized answer
        lines.append("## Findings")
        lines.append("")
        lines.append(report.synthesized_answer)
        lines.append("")
        
        # References
        lines.append("## References")
        lines.append("")
        for ref in report.references:
            formatted_ref = ref.format_vancouver_style()
            lines.append(f"{ref.number}. {formatted_ref}")
        lines.append("")
        
        # Methodology
        if report.methodology_note:
            lines.append("## Methodology")
            lines.append("")
            lines.append(report.methodology_note)
            lines.append("")
        
        # Technical details
        lines.append("## Technical Details")
        lines.append("")
        lines.append("This report was generated using the BMLibrarian multi-agent system:")
        lines.append("")
        lines.append("1. **Query Generation:** Natural language question converted to database query")
        lines.append("2. **Document Retrieval:** PostgreSQL full-text search with pgvector extension")
        lines.append("3. **Relevance Scoring:** AI-powered document scoring (1-5 scale)")
        lines.append("4. **Citation Extraction:** Relevant passage extraction from high-scoring documents")
        lines.append("5. **Report Synthesis:** Medical publication-style report generation")
        lines.append("")
        lines.append("**AI Models Used:**")
        lines.append("- Document scoring and citation extraction: LLM via Ollama")
        lines.append("- Report synthesis: Medical writing-focused language model")
        lines.append("")
        lines.append("**Quality Controls:**")
        lines.append("- Document ID verification prevents citation hallucination")
        lines.append("- Evidence strength assessment based on citation quality and quantity")
        lines.append("- Human-in-the-loop validation at each processing step")
        
        return "\n".join(lines)
    
    def run_complete_workflow(self):
        """Execute the complete research workflow with user interaction."""
        try:
            # Setup
            if not self.setup_agents():
                print("\n‚ùå Cannot proceed without proper agent setup.")
                return
            
            # Start orchestrator
            self.orchestrator.start_processing()
            
            # Step 1: Get research question
            question = self.get_user_question()
            
            # Step 2: Search documents using QueryAgent
            documents = self.search_documents_with_review(question)
            if not documents:
                print("‚ùå Cannot proceed without documents.")
                return
            
            # Step 3: Display and review documents
            documents = self.display_documents(documents)
            if not documents:
                return
            
            # Step 4: Score documents using DocumentScoringAgent
            scored_docs = self.score_documents_with_review(question, documents)
            if not scored_docs:
                print("‚ùå Cannot proceed without scored documents.")
                return
            
            # Step 5: Extract citations using CitationFinderAgent
            citations = self.extract_citations_with_review(question, scored_docs)
            if not citations:
                print("‚ùå Cannot proceed without citations.")
                return
            
            # Step 6: Generate report using ReportingAgent
            report = self.generate_final_report(question, citations)
            if not report:
                print("‚ùå Report generation failed.")
                return
            
            # Step 7: Save report
            while True:
                save_choice = input("\nüíæ Save report as markdown file? (y/n): ").strip().lower()
                if save_choice in ['y', 'yes']:
                    self.save_report_as_markdown(report)
                    break
                elif save_choice in ['n', 'no']:
                    print("Report not saved.")
                    break
                else:
                    print("Please enter 'y' or 'n'.")
            
            # Final summary
            print(f"\nüéâ Research workflow completed successfully!")
            print(f"   Question: {question[:60]}...")
            print(f"   Documents found: {len(documents)}")
            print(f"   Documents scored: {len(scored_docs)}")
            print(f"   Citations extracted: {len(citations)}")
            print(f"   Evidence strength: {report.evidence_strength}")
            
        except KeyboardInterrupt:
            print(f"\n\n‚èπÔ∏è  Workflow interrupted by user.")
        except Exception as e:
            print(f"\n‚ùå Workflow error: {e}")
            import traceback
            traceback.print_exc()
        finally:
            if self.orchestrator:
                self.orchestrator.stop_processing()
    
    def show_configuration_menu(self):
        """Show and modify configuration settings."""
        while True:
            print(f"\n‚öôÔ∏è  Configuration Settings")
            print("=" * 40)
            print(f"1. Max search results: {self.max_search_results}")
            print(f"2. Timeout duration: {self.timeout_minutes} minutes")
            print(f"3. Score threshold: {self.default_score_threshold}")
            print(f"4. Min relevance: {self.default_min_relevance}")
            print(f"5. Documents display limit: {self.max_documents_display}")
            print("6. Reset to defaults")
            print("7. Back to main menu")
            
            choice = input("\nChoose option (1-7): ").strip()
            
            if choice == '1':
                try:
                    new_max = int(input(f"Enter max search results (current: {self.max_search_results}): "))
                    if 1 <= new_max <= 1000:
                        self.max_search_results = new_max
                        print(f"‚úÖ Max search results set to {new_max}")
                    else:
                        print("‚ùå Please enter a number between 1 and 1000")
                except ValueError:
                    print("‚ùå Please enter a valid number")
            
            elif choice == '2':
                try:
                    new_timeout = float(input(f"Enter timeout in minutes (current: {self.timeout_minutes}): "))
                    if 0.5 <= new_timeout <= 30:
                        self.timeout_minutes = new_timeout
                        print(f"‚úÖ Timeout set to {new_timeout} minutes")
                    else:
                        print("‚ùå Please enter a number between 0.5 and 30 minutes")
                except ValueError:
                    print("‚ùå Please enter a valid number")
            
            elif choice == '3':
                try:
                    new_threshold = float(input(f"Enter score threshold (current: {self.default_score_threshold}): "))
                    if 0 <= new_threshold <= 5:
                        self.default_score_threshold = new_threshold
                        print(f"‚úÖ Score threshold set to {new_threshold}")
                    else:
                        print("‚ùå Please enter a number between 0 and 5")
                except ValueError:
                    print("‚ùå Please enter a valid number")
            
            elif choice == '4':
                try:
                    new_relevance = float(input(f"Enter min relevance (current: {self.default_min_relevance}): "))
                    if 0 <= new_relevance <= 1:
                        self.default_min_relevance = new_relevance
                        print(f"‚úÖ Min relevance set to {new_relevance}")
                    else:
                        print("‚ùå Please enter a number between 0 and 1")
                except ValueError:
                    print("‚ùå Please enter a valid number")
            
            elif choice == '5':
                try:
                    new_display = int(input(f"Enter documents display limit (current: {self.max_documents_display}): "))
                    if 1 <= new_display <= 50:
                        self.max_documents_display = new_display
                        print(f"‚úÖ Documents display limit set to {new_display}")
                    else:
                        print("‚ùå Please enter a number between 1 and 50")
                except ValueError:
                    print("‚ùå Please enter a valid number")
            
            elif choice == '6':
                self.max_search_results = 100
                self.timeout_minutes = 5
                self.default_score_threshold = 2.5
                self.default_min_relevance = 0.7
                self.max_documents_display = 10
                print("‚úÖ All settings reset to defaults")
            
            elif choice == '7':
                break
            
            else:
                print("‚ùå Invalid option. Please choose 1-7.")
    
    def show_main_menu(self):
        """Display main menu and handle user choices."""
        while True:
            print(f"\nüè• BMLibrarian Medical Research CLI")
            print("=" * 50)
            print("1. Start new research workflow")
            print("2. Resume previous session")
            print("3. Test system connections")
            print("4. Configuration settings")
            print("5. View documentation")
            print("6. Exit")
            
            choice = input("\nChoose option (1-6): ").strip()
            
            if choice == '1':
                self.run_complete_workflow()
            elif choice == '2':
                print("‚ùå Session resume not implemented yet.")
            elif choice == '3':
                self.setup_agents()
            elif choice == '4':
                self.show_configuration_menu()
            elif choice == '5':
                self.show_documentation()
            elif choice == '6':
                print("üëã Goodbye!")
                break
            else:
                print("‚ùå Invalid option. Please choose 1-6.")
    
    def show_documentation(self):
        """Show basic documentation and help."""
        print(f"\nüìö BMLibrarian CLI Documentation")
        print("=" * 50)
        print()
        print("This CLI provides an interactive workflow for evidence-based medical research:")
        print()
        print("1. **Research Question:** Enter your medical research question")
        print("2. **Query Generation:** AI generates database query with human editing")
        print("3. **Document Search:** Execute search and review results")
        print("4. **Relevance Scoring:** AI scores documents (1-5) for relevance")
        print("5. **Citation Extraction:** Extract relevant passages from high-scoring documents")
        print("6. **Report Generation:** Create medical publication-style report")
        print("7. **Export:** Save report as markdown file")
        print()
        print("**Requirements:**")
        print("‚Ä¢ PostgreSQL database with biomedical literature")
        print("‚Ä¢ Ollama service running locally (http://localhost:11434)")
        print("‚Ä¢ Models: gpt-oss:20b, medgemma4B_it_q8:latest")
        print()
        print("**Tips:**")
        print("‚Ä¢ Be specific in your research questions")
        print("‚Ä¢ Review and edit generated queries for better results")
        print("‚Ä¢ Adjust score thresholds based on your needs")
        print("‚Ä¢ Higher thresholds = fewer but more relevant results")
        print()
        
        input("Press Enter to continue...")


def parse_args():
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(
        description="BMLibrarian Interactive Medical Literature Research CLI",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python bmlibrarian_cli.py                    # Default settings
  python bmlibrarian_cli.py --max-results 50  # Limit search to 50 documents  
  python bmlibrarian_cli.py --timeout 10      # Set 10-minute timeout
  python bmlibrarian_cli.py --quick           # Quick testing mode (20 results, 2 min timeout)
        """
    )
    
    parser.add_argument(
        '--max-results', 
        type=int, 
        default=100,
        metavar='N',
        help='Maximum number of search results to retrieve (default: 100)'
    )
    
    parser.add_argument(
        '--timeout',
        type=float,
        default=5.0,
        metavar='M',
        help='Timeout duration in minutes for report generation (default: 5.0)'
    )
    
    parser.add_argument(
        '--score-threshold',
        type=float,
        default=2.5,
        metavar='S',
        help='Default document score threshold (default: 2.5)'
    )
    
    parser.add_argument(
        '--min-relevance',
        type=float,
        default=0.7,
        metavar='R',
        help='Default minimum citation relevance (default: 0.7)'
    )
    
    parser.add_argument(
        '--quick',
        action='store_true',
        help='Quick testing mode (20 results, 2-minute timeout, lower thresholds)'
    )
    
    return parser.parse_args()


def main():
    """Main entry point for the CLI application."""
    try:
        # Parse command line arguments
        args = parse_args()
        
        # Create CLI with custom settings
        cli = MedicalResearchCLI()
        
        # Apply command line arguments
        if args.quick:
            cli.max_search_results = 20
            cli.timeout_minutes = 2
            cli.default_score_threshold = 2.0
            cli.default_min_relevance = 0.6
            print("üöÄ Quick testing mode enabled!")
        else:
            cli.max_search_results = args.max_results
            cli.timeout_minutes = args.timeout
            cli.default_score_threshold = args.score_threshold
            cli.default_min_relevance = args.min_relevance
        
        # Show current configuration if non-default
        if (args.max_results != 100 or args.timeout != 5.0 or 
            args.score_threshold != 2.5 or args.min_relevance != 0.7 or args.quick):
            print(f"\n‚öôÔ∏è  Configuration:")
            print(f"   Max search results: {cli.max_search_results}")
            print(f"   Timeout: {cli.timeout_minutes} minutes")
            print(f"   Score threshold: {cli.default_score_threshold}")
            print(f"   Min relevance: {cli.default_min_relevance}")
        
        cli.show_main_menu()
    except KeyboardInterrupt:
        print(f"\n\nüëã Exiting BMLibrarian CLI...")
    except Exception as e:
        print(f"\n‚ùå Fatal error: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()